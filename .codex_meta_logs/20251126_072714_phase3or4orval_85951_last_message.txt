Baseline snapshot: `output/metrics/anytime_td/xgboost/20251126_072200_c4fc638_cutoff_h090m/metrics.yaml` shows auc 0.8432, pr_auc 0.2774, brier 0.0849, precision_at_thresh 0.2462; feature_importance dominated by drive_hist_* and pred_efficiency_tds. Rebuild command (`venv/bin/python main.py`) failed twice (timed out while VisualCrossing requests were blocked by restricted network), so no new metrics directory was produced; `output/metrics/anytime_td/xgboost` latest remains 20251126_072200.

Findings vs plan:
- Labels & semantics: only a single `anytime_td` label derived from `touchdowns > 0` in `utils/feature/player_game_level.py` with fallback `_total_touchdowns` sum; no option A/B semantics, no offense vs all split, no label versioning/config toggle in `config/training.yaml`, no design doc or edge-case audit, no synthetic unit test.
- Explicit label columns/target wiring: `utils/feature/targets.py` still hardcodes `anytime_td`; training uses `target_col: anytime_td` without choice or warnings. No label audit script/notebook or tests exercising TD type edge cases.
- Leakage guardrails: `LEAK_PRONE_COLUMNS` is a short static list of obvious same-game stats; no pattern-based rules or per-problem allowlists, and no fail-fast enforcement in `pipeline/feature.py`/`pipeline/train.py`. Feature selection relies on prefixes in `config/training.yaml` without validation of suspicious columns; tests in `tests/test_data_leak_guard.py` only check a tiny banned set. No schema snapshot/artifact is written during feature builds.
- Decision-time realism for odds/injuries: Market columns (`market_anytime_td_*`) are built for multiple horizons in `utils/feature/player_game_level.py` but no horizon selection or cutoff enforcement; as-of metadata isn’t consulted to drop post-cutoff snapshots. Injury features aren’t checked against snapshot timestamps; no assertions that external joins happen before decision cutoff. Training config lacks odds_horizon or injury snapshot options.
- CV/purge leakage: `config/training.yaml` sets `purge_td: 0` and uses `group_col: game_id`; no purge window for player-level leakage via rolling features, no test covering PurgedGroupTimeSeriesSplit behavior.
- Football-grounded features: Missing inside-5/inside-10 usage, 2-minute/hurry-up targets, alignment-based red-zone route shares, and richer drive state buckets; `_finalize_drive_history_features` only computes generic prev/l3 aggregates. Role flags in `utils/feature/player_game_level.py` are simple quantiles on two shares, not context-aware. Injury availability features don’t estimate snap share/next-man-up expectations.
- Team context/scoring: Team total adjustment exists (`utils/train/team_total.py`) but feature set only includes `team_implied_total`/`spread_line`; no NFL-specific scoring environment refresh or multipliers combining usage with team scoring rates.
- Legacy/dead paths: `pipeline/train.py` still carries MLB artifacts (`game_pk`, `inning_topbot`) in calibration/conformal paths; docs under `docs/Rolling.md`, `docs/statcast_glossary.md`, and tests reference MLB schemas. `tune_features` CLI is effectively a stub (fixed search space, relies on pipeline/feature CLI that lacks those params) and not wired end-to-end. `snap_zero_usage_stub` remains as a commented-out placeholder in `utils/feature/player_game_level.py`. No canonicalization of `game_id` vs `game_pk`.
- Documentation/monitoring: No docs capturing label semantics or leak rules (no `docs/anytime_td_pipeline.md`). Evaluation slices by position/role/odds buckets are absent; no odds-free baseline model path; no continuous leak audit artifacts or CI smoke tests on a small subset.

Because the rebuild run could not complete, post-run metric comparison and updated feature importance are unavailable; `output/metrics/cutoff_backtest_summary.csv` still reflects older runs (latest row matches 20251126_072200).