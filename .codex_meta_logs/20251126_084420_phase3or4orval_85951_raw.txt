OpenAI Codex v0.63.0 (research preview)
--------
workdir: /Users/jameswest/Desktop/nfl_predict
model: gpt-5.1-codex-max
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: none
reasoning summaries: auto
session id: 019ac10d-3a98-77a3-a2c3-f90865e84620
--------
user
Okay, great. Please work like an agent and implement the following plan in this repository.

    Here is the plan:

    <BEGIN_PLAN_START>
    **Overall Priorities**

- **P0 – Correctness & leakage:** Fix label semantics, explicit target wiring, strict leak guardrails, and decision-time realism for odds/injuries. These are mandatory before trusting any new metrics.
- **P1 – Evaluation & CV:** Add purge-aware CV, odds-free baselines, and slice-level evaluation so we can see where the model is actually good or leaking.
- **P2 – Football-grounded features:** Layer in richer TD-mechanism features (red-zone roles, goal-line usage, hurry-up, etc.) once P0/P1 are solid.
- **P3 – Cleanup & ergonomics:** Remove MLB cruft, canonicalize IDs, wire feature tuning, and upgrade docs/monitoring/CI.

Below is a detailed, implementation-ready plan organized by theme and priority.

---

**1. Labels & Target Semantics (P0)**

Goals: precisely define what “anytime TD” means; capture variants (offense-only, by TD type); make configuration- and version-controlled.

- **1.1 Design label variants and semantics**
  - Define a small, explicit set of label types:
    - `anytime_td_all`: player scores any TD (rushing/receiving/passing/return) on any side of the ball.
    - `anytime_td_offense`: only offensive TDs (rushing/receiving/passing credited to offensive players).
    - `anytime_td_rush`, `anytime_td_rec`, `anytime_td_pass_thrown`: type-specific labels.
    - Optionally `anytime_td_redzone` (TD originating from inside opponent 20), `anytime_td_goal_to_go` (inside 10 or 5) for future experiments.
  - Decide and document how to treat edge cases:
    - TD via laterals, fumble recoveries, fake punts/FGs, pick-sixes for offensive players, 2-point conversions, penalties nullifying TDs.
    - Multi-TD games (label is still binary; separate count label can be added if useful).
  - Produce a short design note listing:
    - Exact inclusion/exclusion rules per label.
    - Required input columns and which upstream tables they come from.

- **1.2 Implement label generation with versioning**
  - In `utils/feature/player_game_level.py`:
    - Refactor current `anytime_td` computation into a reusable helper, e.g. `_compute_td_labels(row, label_cfg)`, that can output multiple label columns.
    - Introduce a small `LabelConfig` structure or dict that encodes which TD types to include and whether to restrict to offensive plays.
  - Add a new module or function (e.g. `utils/feature/labels.py`) that:
    - Defines label “versions” (e.g. `v1_any_offense`, `v2_any_all`, etc.) as combinations of rules.
    - Provides a function `get_label_spec(version_name)` that returns both:
      - Column name(s) to be produced.
      - Rules to feed into `_compute_td_labels`.
  - Add a `label_version` field in `config/training.yaml` under the relevant problem:
    - E.g. `labels: { version: v1_any_offense, target_col: anytime_td_offense }`.
    - Allow multiple label columns per run for multi-task experiments, but keep initial scope simple (one primary target).

- **1.3 Explicit target wiring in training and feature code**
  - In `utils/feature/targets.py`:
    - Replace hardcoded `anytime_td` with logic that:
      - Reads `target_col` (and possibly `label_version`) from config.
      - Ensures the column exists in the assembled feature frame before training.
      - Raises a clear error if the configured target is missing or NaN-heavy.
  - In `pipeline/train.py`:
    - Make `target_col` mandatory in the training section.
    - Assert that only label columns from the configured version are passed into `y`.
    - Optionally log label prevalence and positive rate per run.

- **1.4 Label audit tests and fixtures**
  - Add a very small synthetic dataset (e.g. CSV or in-memory DataFrame) with carefully constructed plays:
    - Simple rushing TD, receiving TD, passing-TD-thrower, defensive player scoring off turnover, special teams TD, nullified TD, etc.
  - Create unit tests (under `tests/labels/` or similar) that:
    - Run label generation for each version.
    - Assert expected 0/1 labels per player-game for each scenario.
  - Add a basic “label sanity” script/notebook to:
    - Show positive rate by position, team, and season.
    - Flag obviously inconsistent patterns (e.g. kickers with many positive labels, players labeled positive with 0 snaps).

- **Risks / Pitfalls**
  - Backward compatibility: old runs will have old semantics; plan to tag runs with `label_version` for clarity.
  - Label proliferation: keep to a small, curated set of versions to avoid confusion and combinatorial explosion.

---

**2. Leakage Guardrails & Schema Tracking (P0)**

Goals: systematically prevent data leakage, make prohibited columns/config leaks fail loudly, and track feature schemas per run.

- **2.1 Expand and structure leak-prone column policies**
  - Replace simple `LEAK_PRONE_COLUMNS` list with structured policies:
    - Patterns (e.g. `*_tds`, `points_scored`, `final_score`, `postgame_*`).
    - Time-suffix patterns that are clearly post-outcome (e.g. `_fullgame`, `_final`).
    - Direct target relatives (`anytime_td`, `*_touchdowns`, `*_td_share` after the game).
  - Define per-problem allowlists:
    - E.g. for `anytime_td`, only allow feature prefixes that are plausible pregame or pre-decision (odds, depth chart, usage history).
  - Implement a helper (e.g. `utils/feature/leak_guard.py`) that:
    - Given a features DataFrame and problem config, returns:
      - Allowed columns.
      - Banned columns (with reasons).
    - Raises if any banned column is present or if no allowed features remain.

- **2.2 Integrate guardrails into the feature pipeline**
  - In `pipeline/feature.py` (or wherever feature assembly occurs):
    - After all joins/engineering, but before saving:
      - Compute and log lists of (a) all columns, (b) banned candidates, (c) final kept columns.
      - Drop any non-whitelisted columns and assert that nothing on the banned list makes it into the final dataset.
  - In `pipeline/train.py`:
    - Before fitting the model:
      - Run a guardrail check again on `X_train`/`X_valid` to catch any path where features were loaded from disk without going through assembly guardrails.

- **2.3 Schema snapshot artifacts**
  - For each feature build run:
    - Write a schema artifact that includes for each column:
      - `name`, `dtype`, `source_table`, `tags` (e.g. `odds`, `injury`, `rolling_usage`, `postgame_risk`), and whether it passed guardrails.
    - Save as `output/schema/anytime_td/<model>/<timestamp>_schema.json` (or YAML).
  - Use `label_version`, `feature_version`, and `leak_policy_version` fields in the artifact metadata for long-term traceability.

- **2.4 Leak tests**
  - Extend `tests/test_data_leak_guard.py`:
    - Add pattern-based cases:
      - Simulate columns like `final_score`, `postgame_anytime_td`, `actual_td_count`, etc., and assert they are banned.
    - Add a test that:
      - Mocks a feature DataFrame including both allowed and banned columns, runs guardrail, and checks:
        - Banned columns are dropped or cause failure (depending on desired behavior).
        - At least one allowed feature remains.
  - Add one integration-style test:
    - Create a tiny toy pipeline call that produces features.
    - Assert that the saved schema artifact matches expectations and has no leak-prone columns.

- **Risks / Pitfalls**
  - Over-aggressive banning could drop genuinely useful but safe features; start with a strict core list and allow opt-in overrides via config for expert-level use.
  - Need to keep pattern-based rules in sync with naming conventions as features evolve.

---

**3. Decision-Time Realism: Odds & Injuries (P0)**

Goals: ensure that odds and injury data respect a defined as-of time (e.g. 90 minutes before kick), and that only features available at decision time are used.

- **3.1 Represent as-of timestamps and horizons**
  - Establish a standard as-of representation:
    - For odds: `market_anytime_td_{horizon}_*` where `horizon` encodes offset (e.g. `h090m`).
    - For injuries: snapshot types like `inactives_90m`, `practice_report`, `final_inactives`.
  - In `config/training.yaml`:
    - Add fields under the anytime TD problem:
      - `decision_cutoff_minutes_before_kick: 90`
      - `odds_horizon: h090m`
      - `injury_snapshot: inactives_90m` (or `latest_before_cutoff`).

- **3.2 Odds feature alignment**
  - In `utils/feature/player_game_level.py` where `market_anytime_td_*` features are built:
    - Ensure multiple snapshots are either:
      - Materialized with an explicit horizon suffix, or
      - Filtered down to only the snapshot that matches `odds_horizon`.
  - In the feature selection step:
    - Only include columns with the configured horizon suffix for odds features.
    - Optionally add a `use_odds_features: true/false` flag in training config to run:
      - Baseline without odds (P1).
      - Full model with odds.

- **3.3 Injury and depth chart features**
  - For injury features:
    - Ensure each injury record has a timestamp or report type that can be compared to game kickoff.
    - In the join logic:
      - Filter to records with `report_time <= decision_time`.
      - If multiple, choose the most recent pre-cutoff record per player-game.
  - For depth chart/snap expectation features:
    - Use only historical games and pregame reports up to the previous game; never use actual snaps or usage from the current game.
    - Tag these features as `pre_decision` in schema artifacts.

- **3.4 Decision-time assertions and tests**
  - In the odds/injury join code:
    - Add assertions that:
      - No record with timestamp > decision time is included.
      - At least one record exists for games where data should be available; log missing coverage.
  - Add tests that:
    - Build a tiny dataset with:
      - A pre-cutoff odds snapshot and a post-cutoff snapshot.
      - Multiple injury reports per player (limited and then full participant closer to game).
    - Verify that:
      - Only the correct horizon/snapshot is included.
      - Changing `decision_cutoff_minutes_before_kick` or `odds_horizon` changes which columns appear.

- **Risks / Pitfalls**
  - Historical data may lack fine-grained timestamps for some sources; define conservative assumptions (e.g. treat unknown-time injury reports as post-cutoff unless known safe) and document them.
  - Misconfigured horizons can silently eliminate odds data; add logging that counts how many games/players have odds at the chosen horizon.

---

**4. Cross-Validation & Purge Leakage (P1 but important)**

Goals: prevent leakage via time and groups, especially with rolling features, and make CV reflect real deployment.

- **4.1 Formalize time and group keys**
  - Decide on standard split keys:
    - `group_col`: `game_id` or `game_id + team` depending on how data is structured.
    - `time_col`: kickoff datetime (not just season/week, if available).
  - Add fields to `config/training.yaml`:
    - `time_col`, `group_col`.
    - `purge_window_days` or `purge_window_games`.
    - Optional `embargo_days` separating train and validation.

- **4.2 Implement PurgedGroupTimeSeriesSplit**
  - Encapsulate the splitting logic in a helper (e.g. `utils/cv/purged_group_tscv.py`):
    - Wrap existing PurgedGroupTimeSeriesSplit or implement equivalent.
    - Inputs: time, group, purge window, embargo.
    - Output: list of `(train_idx, valid_idx)` pairs.
  - In `pipeline/train.py`:
    - Use this helper whenever CV is configured for time-series problems.
    - Ensure folds are monotone in time and respect group boundaries.

- **4.3 Align purge window with rolling windows**
  - Determine the maximum lookback window used for rolling features (e.g. last 5 games, last 8 weeks).
  - Set `purge_window` to at least that lookback window in terms of time or game count:
    - If using last 4 weeks of stats, purge at least 4 weeks before each validation window.
  - Document this relationship in the config and docs so future feature changes adjust purge accordingly.

- **4.4 Tests for leakage via CV**
  - Add tests that:
    - Construct a small timeline with player-game rows, e.g. 10 games.
    - Use fake rolling stats that depend on latest game to simulate potential leakage.
    - Run PurgedGroupTimeSeriesSplit and assert:
      - No validation rows share `game_id` or time window overlap with training rows inside the purge window.
      - The earliest training date in a fold is at least `purge_window` before the earliest validation date in that fold.

- **Risks / Pitfalls**
  - Larger purge windows can reduce effective training size; monitor fold sizes and adjust number of folds or lookback windows accordingly.
  - Mis-specified `time_col` (e.g. using data ingestion time instead of game time) can give misleading splits.

---

**5. Football-Grounded Feature Engineering (P2)**

Goals: add features that directly reflect the mechanisms by which TDs occur: red-zone roles, goal-line roles, hurry-up usage, designed touches, etc.

- **5.1 Red-zone and goal-line usage**
  - From play-by-play / drive data:
    - Compute per-player rates and shares:
      - Inside-20, inside-10, inside-5 carries and targets.
      - Goal-to-go carries/targets.
      - Draw a distinction between passing TDs and rush TDs.
    - Aggregate at season-to-date and recent window (last 3–5 games).
  - In `utils/feature/player_game_level.py` (or new dedicated module):
    - Add features such as:
      - `rz_carry_share_season`, `rz_target_share_season`.
      - `goal_line_carry_share_recent`, `goal_line_target_share_recent`.
      - Binary flags like `primary_goal_line_back`, `primary_rz_receiver` based on quantile thresholds within team.

- **5.2 2-minute and hurry-up roles**
  - Identify 2-minute / hurry-up situations:
    - Last 2 minutes of each half.
    - Situations where offense is down by more than one score late in game.
  - Compute per-player:
    - Snap share or target share in these situations.
    - TDs scored in hurry-up vs normal situations.
  - Add features:
    - `two_minute_snap_share`, `hurry_up_target_share`, etc.
    - Flags for “two-minute back” or “hurry-up slot WR”.

- **5.3 Alignment and route role features (if data available)**
  - If route/alignments exist:
    - Rate at which a player:
      - Lines up in the slot vs outside vs backfield vs inline TE in the red zone.
      - Runs routes in the end zone / inside 10.
  - Features:
    - `rz_slot_route_share`, `rz_inline_te_route_share`, `end_zone_target_share`.
    - Flags like `primary_end_zone_target`.

- **5.4 QB-specific TD mechanisms**
  - For QBs:
    - Designed rush attempts inside 10 / 5.
    - Scramble TD rates relative to dropbacks.
    - RZ pass vs scramble tendencies.
  - Features:
    - `qb_rz_rush_share`, `qb_scramble_td_rate`, `qb_rz_pass_td_rate`.

- **5.5 Running backs & TEs vs WRs**
  - Add position-specific feature templates:
    - For RBs: early-down vs short-yardage usage; RZ carries; screen target share.
    - For TEs: red-zone target share, end-zone target share, block vs route ratio.
    - For WRs: deep target share vs red-zone target share; contested catch targets near end zone.

- **5.6 Risk-aware implementation**
  - Tag all new features with:
    - `source` (drive/play-by-play), `time_window` (season vs last N games), and `situation` (red_zone, hurry_up).
  - Ensure they are built strictly from past games only; no use of current-game outcomes.
  - Add a small suite of unit tests for feature aggregation logic using synthetic play data.

- **Risks / Pitfalls**
  - Data quality: mis-tagged field position or missing drives could bias red-zone features; add sanity checks (e.g. team total sums of red-zone carries roughly match play counts).
  - Overfitting: many niche situational features; use regularization and monitor importance to prune low-value features later.

---

**6. Team Context & Scoring Environment (P2)**

Goals: connect player TD propensity to team scoring expectations and game environment.

- **6.1 Improve team scoring features**
  - From betting lines and historical scoring:
    - Compute `team_implied_total` and `opp_implied_total` from spreads and totals if not already robustly done.
    - Compute league-average implied total per season/week to create normalized features:
      - `team_implied_total_z_score`.
  - Add features that tie player usage to team context:
    - `team_implied_td_count ≈ team_implied_total / 7`.
    - Player share features:
      - `player_rush_td_share_recent * team_implied_td_count`.
      - `player_rec_td_share_recent * team_implied_td_count`.

- **6.2 Game environment features**
  - Compute:
    - Pace (plays per game, situation-adjusted if possible).
    - Run/pass rate, particularly in red zone.
    - Weather and stadium type (indoors/outdoors, surface) if available.
  - Add features:
    - `fast_pace_flag`, `run_heavy_rz_flag`, `indoor_flag`, `good_weather_flag`.

- **6.3 Calibration and interaction*
  - Later (P3): consider modeling team-level TD expectations separately and feeding them as a feature or constraint into the player-level model.
  - For now, ensure:
    - Team context variables are present and not collinear with straightforward derived totals.

- **Risks / Pitfalls**
  - Team implied totals are odds-derived and may overlap conceptually with player-specific odds; track both but be careful about double-counting odds information in leak analyses.
  - Environment features can be noisy; monitor importance and prune if they add little.

---

**7. Cleanup of Legacy / Dead Paths (P0–P3 depending on impact)**

Goals: remove MLB remnants and dead code that confuse maintenance and hide bugs.

- **7.1 Canonicalize game identifiers**
  - Decide on canonical ID (likely `game_id`).
  - Sweep code for `game_pk` and other MLB-centric IDs:
    - In `pipeline/train.py`, calibration/conformal code, and any schema definitions.
  - Plan:
    - Introduce a small mapping function (e.g. `canonical_game_id`) if necessary.
    - Migrate any remaining MLB references in NFL paths to the canonical ID.

- **7.2 Remove MLB-specific artifacts from NFL pipeline**
  - Identify MLB-only references:
    - `inning_topbot`, `statcast`, `pitcher_id`, etc.
  - For NFL-specific code paths:
    - Strip out MLB-only parameters and data paths.
    - Update docs and tests to only reference NFL-oriented concepts.
  - Keep MLB code only if there is a separate MLB pipeline clearly separated by directory or namespace; otherwise archive or remove.

- **7.3 Fix or deprecate `tune_features` CLI**
  - Decide on intended behavior:
    - Either wire it to an actual feature search (e.g. toggling feature groups, evaluating metrics) or explicitly remove/deprecate it to avoid confusion.
  - If keeping:
    - Make it call the main pipeline with different feature set configurations.
    - Add minimal documentation in `docs/` explaining how to use it and its limitations.

- **7.4 Remove commented stubs like `snap_zero_usage_stub`**
  - If the concept is still valuable:
    - Either implement properly (e.g. “zero-usage but active players” flag) with tests, or explicitly move to a tracked TODO in documentation.
  - Remove commented-out code to keep the feature module readable.

- **7.5 Documentation cleanup**
  - Move MLB-specific docs (`docs/Rolling.md`, `docs/statcast_glossary.md`) to:
    - A clearly-labeled MLB section, or
    - A deprecated folder if not relevant.
  - Add a short note in any shared docs clarifying which parts are NFL vs MLB.

- **Risks / Pitfalls**
  - Some code may be shared between sports; be careful not to break generic utilities while removing MLB-specific logic.
  - Ensure tests covering MLB paths (if any) are either updated or clearly separated.

---

**8. Documentation & Monitoring (P1)**

Goals: capture label/feature/leak rules in living docs and set up monitoring that can be automated.

- **8.1 Label and pipeline documentation**
  - Create `docs/anytime_td_pipeline.md` containing:
    - High-level overview of the anytime TD problem.
    - Label semantics, versions, and their exact definitions.
    - Major feature groups and their intended football meaning.
    - Leak guardrail rules and example banned columns.
    - Configuration fields specific to anytime TD (label_version, odds_horizon, purge_window, etc.).
  - Include a short “How to add new features safely” section:
    - Steps to add new features, tag them, and run leak and label tests.

- **8.2 Evaluation and slicing**
  - Extend evaluation to produce:
    - Metrics by position (QB/RB/WR/TE/other).
    - Metrics by role flags (goal-line back, primary red-zone target).
    - Metrics by odds buckets (e.g. implied anytime TD probability deciles).
    - Metrics by team implied total buckets.
  - Add outputs to `output/metrics/anytime_td/...`:
    - E.g. `metrics_by_position.csv`, `metrics_by_odds_bucket.csv`.

- **8.3 Odds-free baseline and benchmarks**
  - Add an odds-free model config:
    - Same labels and non-odds features only.
  - Track its metrics alongside the full model in:
    - `output/metrics/cutoff_backtest_summary.csv`.
  - Use this baseline to detect suspicious jumps when odds are added; large improvements may indicate misuse of post-decision data.

- **8.4 CI / smoke tests**
  - Add a lightweight CI test (using a small subset of games) that:
    - Runs feature assembly for a small date range.
    - Runs training for 1–2 folds.
    - Confirms:
      - No leak guardrail violations.
      - Labels are generated and have reasonable prevalence.
      - Model trains successfully and produces artifacts.
  - Ensure these tests do not depend on external network I/O to avoid flakiness.

- **Risks / Pitfalls**
  - Overly heavy CI can become slow; keep smoke tests tiny and separate from full offline backtests.
  - Slicing metrics produce many files; keep a clear naming scheme and document where to find them.

---

**9. Implementation & Rollout Strategy**

- **Phase P0: Safety and semantic correctness**
  - Implement label versioning and target wiring.
  - Set up leak guardrails and schema snapshots.
  - Enforce decision-time realism for odds and injuries.
  - Clean the most egregious MLB cruft in NFL paths and canonicalize `game_id`.
  - Run a small backtest (once network and runtime constraints allow) to ensure metrics are reasonable and no guardrail violations occur.

- **Phase P1: CV, baselines, and richer evaluation**
  - Introduce purge-aware time-series CV.
  - Add odds-free baseline config and evaluation.
  - Implement evaluation slices by position/role/odds bucket.
  - Write core docs for labels and leak policies; add CI smoke tests.

- **Phase P2: Football-grounded feature expansion**
  - Add red-zone and goal-line usage features, hurry-up roles, and position-specific TD mechanisms.
  - Integrate team context and scoring environment features.
  - Retrain and evaluate; compare improvements vs odds-free and full models while monitoring for new leak risks.

- **Phase P3: Ergonomics and tuning**
  - Finalize cleanup of dead CLIs and MLB docs in NFL scope.
  - Decide on and implement `tune_features` functionality or deprecate it.
  - Iterate on documentation, examples, and notebooks for future contributors.

If you’d like, the next step can be to pick a single P0 theme (e.g. label versioning + target wiring) and I can outline concrete code changes per file for that slice.
    <END_PLAN_END>

    Repository + Git requirements:

    - This repo is under git. At the very beginning of THIS RUN, before editing any files:
        1) Run `git status` to inspect the current state.
        2) If there are uncommitted changes from previous runs, stage and commit them with a concise
           message like `codex: iteration checkpoint` (or a slightly more descriptive variant).
        3) If a remote named `origin` exists and authentication allows, run `git push` so that the
           current state is saved remotely. If push fails due to auth or remote issues, continue with
           local commits only, but do NOT delete history.
        4) Only after ensuring there is a clean commit of the current state should you begin making
           new edits in this run.

    - During this run:
        * Make coherent, incremental commits as you reach logical checkpoints.
        * If you realize that your changes have badly broken the project and you cannot fix them
          cleanly within this run, you may revert to the last good commit (for example, using
          `git reset --hard HEAD` or `git checkout .`), then proceed more conservatively.

    Your tools allow you to:
    - Inspect files in this repo.
    - Edit files.
    - Run shell commands/tests as needed (e.g. project-specific tests, evaluations, or scripts).
    - Use git commands to create commits and, if possible, push them.

    Your goal in THIS RUN:
    - Implement as much of the plan as you reasonably can with high quality.
    - Prioritize correctness, alignment with football reality, and improved model performance
      over speed.
    - Run whatever tests or checks are appropriate to validate your changes.

    At the end of THIS RUN:
    - Give a concise update on your progress.
    - If the ENTIRE plan is fully implemented and validated, include a line with EXACTLY:
        The plan is completely implemented
      (case and spacing exactly as written).
    - If the plan is NOT fully implemented, include a concise progress summary starting with:
        PROGRESS:
      followed by a short description of what you accomplished and what remains.

    Take your time and implement the richest / most complete solution for the pieces you touch in this run.

Here is the progress summary from the last implementation run:

<BEGIN_PROGRESS_FROM_PREVIOUS_RUN>
PROGRESS: Leak guardrails and odds-horizon filtering with tests/docs are in place; remaining plan items include decision-time injury alignment, purge-aware CV/slicing, richer football features, and broader docs/evaluation.
<END_PROGRESS_FROM_PREVIOUS_RUN>

Use this to avoid repeating work and to focus on remaining parts of the plan.

mcp startup: no servers
warning: Repository snapshot encountered large untracked directories: .codex_meta_logs (887 files). This can slow Codex; consider adding these paths to .gitignore or disabling undo in your config.

thinking
**Checking git status**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && git status' in /Users/jameswest/Desktop/nfl_predict succeeded in 718ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   codex_meta_loop.py
	modified:   docs/rolling_window_implementation.md
	modified:   pipeline/feature.py
	modified:   pipeline/predict.py
	modified:   pipeline/train.py
	modified:   tests/test_data_leak_guard.py
	modified:   tests/test_training_config_features.py
	modified:   utils/collect/visual_crossing_weather.py
	modified:   utils/collect/weather_forecasts.py
	modified:   utils/feature/player_game_level.py
	modified:   utils/feature/rolling_window.py
	modified:   utils/feature/stats.py
	modified:   utils/feature/targets.py

thinking
**Preparing to commit changes**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && git add .' in /Users/jameswest/Desktop/nfl_predict exited 128 in 713ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
fatal: Unable to create '/Users/jameswest/Desktop/nfl_predict/.git/index.lock': Operation not permitted

thinking
**Checking .git index.lock presence**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && ls -l .git/index.lock' in /Users/jameswest/Desktop/nfl_predict exited 1 in 734ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
ls: .git/index.lock: No such file or directory

thinking
**Analyzing operation permission errors**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && ls -ld .git' in /Users/jameswest/Desktop/nfl_predict succeeded in 731ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
drwxr-xr-x  13 jameswest  staff  416 Nov 26 08:44 .git

thinking
**Investigating git add permission issues**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && touch _tmp_can_write2' in /Users/jameswest/Desktop/nfl_predict succeeded in 738ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted

thinking
**Investigating git add failure**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && touch .git/testwrite' in /Users/jameswest/Desktop/nfl_predict exited 1 in 864ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
touch: .git/testwrite: Operation not permitted

thinking
**Encountering write restrictions on .git directory**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && ls -lO .git' in /Users/jameswest/Desktop/nfl_predict succeeded in 704ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
total 72
-rw-r--r--    1 jameswest  staff  -    18 Nov 26 01:04 COMMIT_EDITMSG
-rw-r--r--    1 jameswest  staff  -    21 Nov 25 22:42 HEAD
-rw-r--r--    1 jameswest  staff  -    41 Nov 25 22:50 ORIG_HEAD
-rw-r--r--    1 jameswest  staff  -   308 Nov 25 22:58 config
-rw-r--r--    1 jameswest  staff  -    73 Nov 25 22:42 description
drwxr-xr-x   15 jameswest  staff  -   480 Nov 25 22:42 hooks
-rw-r--r--    1 jameswest  staff  - 13040 Nov 26 01:04 index
drwxr-xr-x    3 jameswest  staff  -    96 Nov 25 22:42 info
drwxr-xr-x    4 jameswest  staff  -   128 Nov 25 22:45 logs
drwxr-xr-x  260 jameswest  staff  -  8320 Nov 26 03:43 objects
drwxr-xr-x    5 jameswest  staff  -   160 Nov 25 22:58 refs

thinking
**Diagnosing .git write restrictions**

I'm analyzing why writing to the .git directory fails despite read access, considering factors like filesystem overlays, sandbox restrictions, and possible environment-level blocks. I’m exploring if permission flags or macOS-specific protections prevent modifying .git, and thinking about alternative approaches like using separate git directories or relocating the repo, but given the strict policy not to escalate, I may have to flag this inability before proceeding further.
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && ls -lO .git/index' in /Users/jameswest/Desktop/nfl_predict succeeded in 753ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
-rw-r--r--  1 jameswest  staff  - 13040 Nov 26 01:04 .git/index

thinking
**Evaluating git commit restrictions and planning next steps**

The environment seems to prevent writing to the .git directory, blocking staging and committing changes, so I’ll proceed with the plan’s editing tasks without committing and make note of this limitation. I’ll inspect the repo’s current state, especially label-related code, to prioritize P0 tasks like label versioning and target wiring, considering breaking down complex tasks with the plan tool as needed.
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg "anytime_td" utils/feature/player_game_level.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 757ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
- anytime_td_offense (binary): Did player score an offensive TD (rush/rec/pass thrown)?
- anytime_td_all (binary): Did player score any TD (including defensive/special teams if present)?
- anytime_td_rush / anytime_td_rec / anytime_td_pass_thrown: Type-specific TD flags.
- anytime_td (binary): Legacy alias → current label version primary.
    "market_anytime_td_prob",
    "market_anytime_td_consensus",
    "market_anytime_td_spread",
    "market_anytime_td_book_count",
    "market_anytime_td_prob_2h",
    "market_anytime_td_consensus_2h",
    "market_anytime_td_spread_2h",
    "market_anytime_td_book_count_2h",
    "market_anytime_td_prob_6h",
    "market_anytime_td_consensus_6h",
    "market_anytime_td_spread_6h",
    "market_anytime_td_book_count_6h",
    "market_anytime_td_prob_24h",
    "market_anytime_td_consensus_24h",
    "market_anytime_td_spread_24h",

thinking
**Considering file exploration strategies**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "anytime_td" utils/feature/player_game_level.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 732ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
7:- anytime_td_offense (binary): Did player score an offensive TD (rush/rec/pass thrown)?
8:- anytime_td_all (binary): Did player score any TD (including defensive/special teams if present)?
9:- anytime_td_rush / anytime_td_rec / anytime_td_pass_thrown: Type-specific TD flags.
10:- anytime_td (binary): Legacy alias → current label version primary.
90:    "market_anytime_td_prob",
91:    "market_anytime_td_consensus",
92:    "market_anytime_td_spread",
93:    "market_anytime_td_book_count",
94:    "market_anytime_td_prob_2h",
95:    "market_anytime_td_consensus_2h",
96:    "market_anytime_td_spread_2h",
97:    "market_anytime_td_book_count_2h",
98:    "market_anytime_td_prob_6h",
99:    "market_anytime_td_consensus_6h",
100:    "market_anytime_td_spread_6h",
101:    "market_anytime_td_book_count_6h",
102:    "market_anytime_td_prob_24h",
103:    "market_anytime_td_consensus_24h",
104:    "market_anytime_td_spread_24h",

thinking
**Examining label computation section**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '3230,3405p' utils/feature/player_game_level.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 716ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
            # Passer can also rush (e.g., Lamar Jackson)
            pl.col("rushing_yards").fill_null(0).sum().cast(pl.Float64).alias("rushing_yards"),
            pl.col("carry").fill_null(0).sum().cast(pl.Int64).alias("carry"),
            pl.when(pl.col("rush_touchdown") == 1).then(1).otherwise(0).sum().cast(pl.Int64).alias("rushing_td_count"),
            pl.col("red_zone_carry").fill_null(0).sum().cast(pl.Int64).alias("red_zone_carry"),
            pl.col("goal_to_go_carry").fill_null(0).sum().cast(pl.Int64).alias("goal_to_go_carry"),
            
            # No receiving for passers typically
            pl.lit(0.0).cast(pl.Float64).alias("receiving_yards"),
            pl.lit(0).cast(pl.Int64).alias("target"),
            pl.lit(0).cast(pl.Int64).alias("reception"),
            pl.lit(0).cast(pl.Int64).alias("receiving_td_count"),
            pl.lit(0).cast(pl.Int64).alias("red_zone_target"),
            pl.lit(0).cast(pl.Int64).alias("goal_to_go_target"),
            
            # Touchdowns (scored by player, not thrown)
            pl.when(pl.col("touchdown_player_id") == pl.col("passer_player_id"))
              .then(pl.col("touchdown").fill_null(0))
              .otherwise(0)

thinking
**Continuing to locate label handling**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '3405,3580p' utils/feature/player_game_level.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 703ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
            pl.col("red_zone_target").fill_null(0).sum().alias("red_zone_target"),
            pl.col("red_zone_carry").fill_null(0).sum().alias("red_zone_carry"),
            pl.col("goal_to_go_target").fill_null(0).sum().alias("goal_to_go_target"),
            pl.col("goal_to_go_carry").fill_null(0).sum().alias("goal_to_go_carry"),
            
            # Sum TD counts
            pl.col("passing_td").fill_null(0).sum().alias("passing_td"),
            pl.col("rushing_td_count").fill_null(0).sum().alias("rushing_td_count"),
            pl.col("receiving_td_count").fill_null(0).sum().alias("receiving_td_count"),
            pl.col("touchdowns").fill_null(0).sum().alias("touchdowns"),
            
            # Team context (take first, should be same)
            pl.col("team").first().alias("team"),
            pl.col("opponent").first().alias("opponent"),
            *_metadata_exprs(df),
        ])
    )
    
    # Fill nulls with 0 for numeric columns

thinking
**Searching for compute_td_labels definition**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "def compute_td_labels" -n utils/feature/player_game_level.py' in /Users/jameswest/Desktop/nfl_predict exited 1 in 714ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted

thinking
**Searching for definition in utils/feature/labels**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "compute_td_labels"' in /Users/jameswest/Desktop/nfl_predict succeeded in 753ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
tests/labels/test_labels.py:3:from utils.feature.labels import compute_td_labels, get_label_spec
tests/labels/test_labels.py:10:def test_compute_td_labels_offense_and_all_variants():
tests/labels/test_labels.py:23:    result = compute_td_labels(df, version="v1_any_offense")
tests/labels/test_labels.py:64:    result = compute_td_labels(df, version="v1_any_all")
utils/feature/labels.py:104:def compute_td_labels(df: pl.DataFrame, *, version: str | None = None) -> pl.DataFrame:
utils/feature/player_game_level.py:40:from utils.feature.labels import DEFAULT_LABEL_VERSION, compute_td_labels
utils/feature/player_game_level.py:3446:    merged = compute_td_labels(merged, version=label_version)

thinking
**Inspecting labels utility**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1,240p' utils/feature/labels.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 721ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
"""Label semantics and versioning for NFL player-level targets.

This module centralizes anytime-TD label definitions so that:
- Semantics are explicit and versioned.
- Training can request a specific label column per problem.
- Feature generation can emit multiple label variants for analysis.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Iterable, Mapping

import polars as pl


@dataclass(frozen=True)
class LabelRules:
    """Rules controlling which TD components count toward a label."""

thinking
**Verifying label versioning and config wiring**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg "label_version"' in /Users/jameswest/Desktop/nfl_predict succeeded in 728ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
utils/feature/player_game_level.py:    label_version: str | None = None,
utils/feature/player_game_level.py:    df_merged = _merge_multi_role_players(df_all, label_version=label_version or DEFAULT_LABEL_VERSION)
utils/feature/player_game_level.py:def _merge_multi_role_players(df: pl.DataFrame, *, label_version: str | None = None) -> pl.DataFrame:
utils/feature/player_game_level.py:    merged = compute_td_labels(merged, version=label_version)
utils/feature/targets.py:    label_version: str | None = None,
utils/feature/targets.py:        if label_version:
utils/feature/targets.py:                spec = get_label_spec(label_version or DEFAULT_LABEL_VERSION)
pipeline/train.py:        self.label_version = None
pipeline/train.py:            self.label_version = problem_copy.get('label_version') or problem_copy.get("labels", {}).get("version")
pipeline/train.py:                label_version=self.label_version,
utils/feature/labels.py:            f"Unknown label_version '{version}'. "
tests/labels/test_labels.py:def test_label_version_any_all_aliases_anytime_td():
pipeline/feature.py:    label_version: str | None = None,
pipeline/feature.py:    label_version = label_version or DEFAULT_LABEL_VERSION
pipeline/feature.py:            build_player_game_level(start_date=cur, end_date=chunk_end, label_version=label_version)
pipeline/feature.py:    spec = get_label_spec(label_version)
pipeline/feature.py:            "label_version": spec.name,
pipeline/feature.py:    label_version: str | None = None,
pipeline/feature.py:                label_version=label_version,

thinking
**Inspecting feature targets script**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1,220p' utils/feature/targets.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 704ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
"""Target column validation for NFL player prediction models.

Target columns are created in utils/feature/player_game_level.py during aggregation.
This module validates that required targets exist and have correct dtypes.
"""

import polars as pl
import logging

from utils.feature.labels import DEFAULT_LABEL_VERSION, get_label_spec

logger = logging.getLogger(__name__)


# Expected target columns and their types
NFL_TARGET_SCHEMA = {
    # Binary classification targets
    "anytime_td": pl.Int8,  # Primary MVP target: did player score a TD?
    "anytime_td_offense": pl.Int8,

thinking
**Checking target column usage in training pipeline**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "target_col" pipeline/train.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 708ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
103:from utils.feature.targets import require_target_column
264:        self.target_col = None
695:            df = self._derive_target_column(
697:                target_col=problem_config["target_col"],
729:    def _derive_target_column(
732:        target_col: str,
744:                  raise ValueError(f"Cannot derive {target_col} from game_by_week (missing source or data).")
752:             merged.rename(columns={source_col: target_col}, inplace=True)
755:             merged = merged[merged[target_col].notna()].reset_index(drop=True)
782:        df[target_col] = (
788:        df = df[df[target_col].notna()].reset_index(drop=True)
967:        X, y = transform_with_artifacts(df, artifacts, self.target_col)
1028:            target_col=self.target_col,
1037:        if self.target_col not in df_train.columns:
1048:        targets = pd.to_numeric(df_train[self.target_col], errors="coerce").fillna(0.0)
1053:                self.target_col: targets,
1069:        actual = grouped[self.target_col].sum()
2220:            self.target_col = problem_copy['target_col']
2233:                target_load_cols.append(self.target_col)

thinking
**Inspecting version label sections**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '2150,2420p' pipeline/train.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 731ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
        try:
            from sklearn.pipeline import Pipeline as _SkPipeline
            if isinstance(model, _SkPipeline):
                if 'clf' in model.named_steps:
                    return model.named_steps['clf']
                else:
                    return model.steps[-1][1]
        except Exception:
            pass

        # 3) Our custom calibration wrappers
        try:
            if isinstance(model, (IsotonicCalibratedModel, BetaCalibratedModel, HistogramBinningCalibratedModel)):
                return model.base_estimator
        except Exception:
            pass

        try:
            if isinstance(model, (IsotonicMeanCalibratedRegressor, SplitConformalRegressor)):

thinking
**Checking training configuration**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1,200p' config/training.yaml" in /Users/jameswest/Desktop/nfl_predict succeeded in 732ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
# config/training.yaml
# Configuration for the model training pipeline (pipeline/train.py)
# NFL Player Prediction Targets

# --- Data I/O ---
data:
  raw_pbp_dir: 'data/raw/pbp_by_day'  # NFL play-by-play raw data
  feature_matrix_path: 'data/processed/final/processed.parquet'  # NFL feature matrix
  
  # Directory to save trained model artifacts (models and feature lists).
  model_output_dir: 'output/models'
  
  # Directory to save evaluation reports and metrics.
  metrics_output_dir: 'output/metrics/'

# =============================================================================
# SHARED COLUMN DEFINITIONS (using YAML anchors to avoid duplication)
# =============================================================================
# These columns leak same-game outcomes and must be excluded from training.

thinking
**Searching for anytime_td in config**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "anytime_td" config/training.yaml' in /Users/jameswest/Desktop/nfl_predict succeeded in 691ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
476:  - name: 'anytime_td'
477:    target_col: 'anytime_td'
668:    - 'anytime_td'
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '460,560p' config/training.yaml" in /Users/jameswest/Desktop/nfl_predict succeeded in 737ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
      - 'total_line'
      - 'team_implied_total'
      - 'travel_rest_days'
      - 'travel_rest_days_l3'
      - 'rest_days_since_last_game'
      - 'team_red_zone_'
      - 'team_goal_to_go_'
      - 'player_red_zone_'
      - 'player_goal_to_go_'
      - 'weather_bad_passing_flag'
    columns_to_discard: *columns_to_discard

  # ============================================================================
  # 4. FINAL COMPOSITION (Anytime TD)
  # ============================================================================
  # Calibrated probability of scoring > 0 TDs.
  - name: 'anytime_td'
    target_col: 'anytime_td'
    label_version: 'v1_any_offense'

thinking
**Reviewing remaining P0 tasks and injury feature location**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "injury" utils/feature/player_game_level.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 706ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
41:from utils.train.injury_availability import (
43:    predict_probabilities as predict_injury_probabilities,
1242:    """Load injury report data for the given years with caching and validation."""
1244:        raise ValueError("No seasons provided for injury load.")
1248:            "nfl_data_py.import_injuries is unavailable; cannot fetch injury data."
1255:        cache_path = INJURY_CACHE_DIR / f"injury_{year}.parquet"
1257:            injury_pl = pl.read_parquet(cache_path)
1260:                injury_pd = import_injuries(years=[year])
1262:                logger.warning("Failed to import injury reports for season %s: %s", year, exc)
1264:            if injury_pd is None or injury_pd.empty:
1265:                logger.info("No injury reports returned for season %s.", year)
1267:            injury_pl = pl.from_pandas(injury_pd)
1268:            if "gsis_id" not in injury_pl.columns:
1271:            injury_pl.write_parquet(cache_path, compression="zstd")
1272:        frames.append(injury_pl)
1277:    injury_pl = pl.concat(frames, how="diagonal_relaxed")
1278:    return injury_pl
1281:def _load_injury_transactions_for_years(years: list[int]) -> pl.DataFrame:
1282:    """Load injury transaction records scraped from prosportstransactions."""

thinking
**Investigating injury decision cutoff logic**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1720,1870p' utils/feature/player_game_level.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 894ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
        df_merged = df_merged.sort(["player_id", "season", "week"])
        df_merged = df_merged.with_columns(
            [
                pl.col(col)
                .shift(1)
                .over("player_id")
                .fill_null(0)
                .alias(f"{col}_prev")
                for col in PRE_SNAP_ROLLING_TARGETS
            ]
        )
        df_merged = df_merged.with_columns(
            [
                pl.col(col)
                .rolling_mean(window_size=3, min_periods=1)
                .over("player_id")
                .shift(1)
                .fill_null(0)
                .alias(f"{col}_l3")

thinking
**Scrolling to decision cutoff**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1870,2055p' utils/feature/player_game_level.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 704ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
                    logger.info("Loaded %d raw injury rows", len(injury_pl))
                    injury_pl = injury_pl.filter(pl.col("player_id").is_not_null())
                    cast_exprs: list[pl.Expr] = [pl.col("player_id").cast(pl.Utf8)]
                    if "season" in injury_pl.columns:
                        cast_exprs.append(pl.col("season").cast(pl.Int32))
                    if "week" in injury_pl.columns:
                        cast_exprs.append(pl.col("week").cast(pl.Int32))
                    if "team" in injury_pl.columns:
                        cast_exprs.append(
                            pl.col("team")
                            .cast(pl.Utf8)
                            .str.strip_chars()
                            .str.to_uppercase()
                            .alias("team")
                        )
                    logger.info("Injury rows after player_id filter: %d", len(injury_pl))

                    status_source_exprs: list[pl.Expr] = []
                    if "report_status_description" in injury_pl.columns:

thinking
**Verifying decision time alignment**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg "decision_cutoff"' in /Users/jameswest/Desktop/nfl_predict succeeded in 719ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
scripts/monitoring/asof_coverage_report.py:    if "decision_cutoff_hours" not in df.columns:
scripts/monitoring/asof_coverage_report.py:        df.group_by("decision_cutoff_hours")
scripts/monitoring/asof_coverage_report.py:        .sort("decision_cutoff_hours")
scripts/monitoring/asof_coverage_report.py:                    h=row["decision_cutoff_hours"],
scripts/monitoring/asof_coverage_report.py:        lines.append("No horizon breakdown available (missing decision_cutoff_hours).")
utils/main/backtest_cutoffs.py:from utils.feature.asof import decision_cutoff_horizons, decision_cutoff_override
utils/main/backtest_cutoffs.py:    horizons = decision_cutoff_horizons()
utils/main/backtest_cutoffs.py:        with decision_cutoff_override(cutoff_hours=hours):
utils/general/__init__.py:    get_decision_cutoff_config,
utils/general/__init__.py:    "get_decision_cutoff_config",
pipeline/train.py:from utils.feature.asof import decision_cutoff_override, fallback_cutoff_hours, get_decision_cutoff_hours
pipeline/train.py:        active_cutoff_hours = float(get_decision_cutoff_hours())
pipeline/train.py:    cutoff_hours = overrides.pop("decision_cutoff_hours", None)
pipeline/train.py:    with decision_cutoff_override(cutoff_hours=cutoff_hours, fallback_hours=fallback_override):
pipeline/train.py:                "decision_cutoff_hours": args.decision_cutoff_hours,
pipeline/feature.py:    decision_cutoff_horizons,
pipeline/feature.py:    decision_cutoff_override,
pipeline/feature.py:    get_decision_cutoff_hours,
pipeline/feature.py:    current_cutoff_hours = float(get_decision_cutoff_hours())

thinking
**Checking for PurgedGroupTimeSeriesSplit implementation**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg "Purged" utils' in /Users/jameswest/Desktop/nfl_predict succeeded in 746ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
utils/train/calibration.py:from .purged_group_time_series_split import PurgedGroupTimeSeriesSplit
utils/train/calibration.py:    splitter = PurgedGroupTimeSeriesSplit(n_splits=max(2, int(cv_folds)), group_gap=int(group_gap))
utils/train/data.py:        raise ValueError("game_id or game_pk column not found, required for PurgedGroupTimeSeriesSplit.")
utils/train/tuning.py:from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
utils/train/tuning.py:        cv = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)
utils/train/tuning.py:        logger.info("Pre-computing CV splits with PurgedGroupTimeSeriesSplit...")
utils/train/tuning.py:        logger.warning(f"PurgedGroupTimeSeriesSplit failed with error: {e}. Falling back to standard TimeSeriesSplit.")
utils/train/purged_group_time_series_split.py:class PurgedGroupTimeSeriesSplit(_BaseKFold):
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1,200p' utils/train/purged_group_time_series_split.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 702ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
# utils/train/purged_group_time_series_split.py
#
# As found here: https://github.com/scikit-learn-contrib/scikit-learn-extra/blob/main/sklearn_extra/model_selection/_split.py
# Included locally to avoid potential dependency issues and allow for customization.

import numpy as np
from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples
from sklearn.utils.validation import _deprecate_positional_args


class PurgedGroupTimeSeriesSplit(_BaseKFold):
    """Time Series cross-validator with non-overlapping, contiguous groups.

    - Produces tail-only, forward-in-time test folds: n_splits equal-sized,
      consecutive test blocks taken from the end of the series.
    - Training for fold i consists of all groups strictly before the test
      block, minus ``group_gap`` groups immediately preceding the test.
    - Optionally supports an explicit fixed ``test_group_size`` (in number of
      groups). If not provided, test size defaults to

thinking
**Examining existing time-series split classes**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "split_data_chronologically" -n pipeline/train.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 695ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
95:from utils.train.data import load_feature_matrix, split_data_chronologically as split_chrono
315:    def split_data_chronologically(self, df):
316:        """Wrapper for split_data_chronologically from utils.train.data."""
2418:            df_train, df_val, df_test = self.split_data_chronologically(df_problem)
2668:            df_train, df_val, _ = trainer.split_data_chronologically(df)
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '300,360p' pipeline/train.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 749ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
        
        # Store predictions + merge keys for downstream problems
        # Format: {problem_name: {"data": DataFrame, "keys": [cols...]}}
        self.problem_predictions: dict[str, dict[str, object]] = {}

        self._ensure_injury_cache()

    def load_data(self, columns=None):
        """Wrapper for load_feature_matrix from utils.train.data."""
        return load_feature_matrix(
            path=self.config['data']['feature_matrix_path'],
            time_col=self.time_col,
            columns=columns
        )

    def split_data_chronologically(self, df):
        """Wrapper for split_data_chronologically from utils.train.data."""
        return split_chrono(
            df=df,

thinking
**Checking data splitting function**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1,260p' utils/train/data.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 880ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
# utils/train/data.py
import pandas as pd
import numpy as np
import logging
import math
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

def load_feature_matrix(path: str, time_col: str, columns=None):
    """Was ModelTrainer.load_data; identical behavior sans 'self'."""
    import pandas as pd
    logger.info(f"Loading feature matrix from {path}...")
    def _read_parquet(**kwargs):
        try:
            return pd.read_parquet(path, **kwargs)
        except Exception as e:
            logger.warning(f"pyarrow.read_parquet failed ({type(e).__name__}: {e}). Retrying with fastparquet…")
            return pd.read_parquet(path, engine="fastparquet", **kwargs)

thinking
**Considering splitting with Purged**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "PurgedGroupTimeSeriesSplit" utils/train/data.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 736ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
34:        raise ValueError("game_id or game_pk column not found, required for PurgedGroupTimeSeriesSplit.")

thinking
**Examining data split logic**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '260,560p' utils/train/data.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 741ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
                len(df_post_local),
                sorted(df_post_local['week'].unique()),
            )

        if not df_unused.empty or not df_post_local.empty:
            df_test_local = pd.concat([df_test_local, df_unused, df_post_local], ignore_index=True)
            df_test_local = df_test_local.sort_values(time_col).reset_index(drop=True)

        logger.info(
            "Rolling split active → train weeks %s (plus prior seasons=%s), validation weeks %s, test weeks %s (season %s).",
            sorted(df_train_latest['week'].unique()),
            bool(len(df_prior_seasons)),
            sorted(df_val_local['week'].unique()),
            sorted(df_test_local['week'].unique()),
            latest_season,
        )

        return df_train_local, df_val_local, df_test_local


thinking
**Integrating purged group time series split**

I’m focusing on implementing the PurgedGroupTimeSeriesSplit within the training pipeline, aligning configuration fields for time, group, and purge windows. I’ll check existing tests related to odds and injury horizon to ensure no data leakage and decide if additional tests or config tweaks are needed. It’s interesting to see partial implementations already—I’ll build on those and possibly add a new helper for cross-validation utilities.
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1,200p' tests/test_odds_horizon_filter.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 801ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
from __future__ import annotations

import pandas as pd
import pytest

train_module = pytest.importorskip("pipeline.train")
ModelTrainer = train_module.ModelTrainer


def test_filter_odds_snapshot_columns_cutoff_keeps_base_only():
    df = pd.DataFrame(
        {
            "market_anytime_td_prob": [0.1, 0.2],
            "market_anytime_td_prob_2h": [0.15, 0.25],
            "market_anytime_td_prob_6h": [0.2, 0.3],
            "other_feature": [1, 2],
        }
    )
    filtered = ModelTrainer._filter_odds_snapshot_columns(df.copy(), "cutoff")

thinking
**Checking test_labels directory**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1,200p' tests/labels/test_labels.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 781ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
import polars as pl

from utils.feature.labels import compute_td_labels, get_label_spec


def _lookup(frame: pl.DataFrame, player_id: str) -> dict:
    return frame.filter(pl.col("player_id") == player_id).to_dicts()[0]


def test_compute_td_labels_offense_and_all_variants():
    df = pl.DataFrame(
        {
            "player_id": ["rush", "rec", "pass", "def"],
            "game_id": ["g1"] * 4,
            "rushing_td_count": [1, 0, 0, 0],
            "receiving_td_count": [0, 1, 0, 0],
            "passing_td": [0, 0, 1, 0],
            # Generic touchdown attribution (e.g., defensive return)
            "touchdowns": [1, 1, 0, 1],

thinking
**Planning multi-step implementation**
Plan update
  → Assess current CV splitting and injury alignment; identify gaps vs plan requirements.
  • Implement purge-aware CV helper/config integration with tests for leakage.
  • Add decision-time injury cutoff enforcement tests and config surface for decision horizons.

thinking
**Evaluating current cross-validation approach**

I’m checking how cross-validation is handled, especially if `PurgedGroupTimeSeriesSplit` is used in training or pipeline files, so I can decide whether to create a new helper or wrap the existing one for purge-aware CV integration. I want to find where CV splitting happens to ensure the new method fits smoothly.
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg "cross" pipeline/train.py' in /Users/jameswest/Desktop/nfl_predict exited 1 in 851ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted

thinking
**Investigating cross-validation usage**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1,220p' utils/train/tuning.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 690ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
# utils/train/tuning.py
from __future__ import annotations
import os, copy, inspect, logging, math
import numpy as np
import optuna
import xgboost as xgb
from typing import Any, Dict, List, Mapping, Tuple
from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, precision_score
from sklearn.model_selection import TimeSeriesSplit
from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit

logger = logging.getLogger(__name__)

DEFAULT_CENTER_WIDTH = 0.5


class TuningConfigError(ValueError):
    """Raised when hyperparameter tuning config is invalid."""


thinking
**Planning extension of purged group time series split**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1,200p' utils/train/calibration.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 696ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
# utils/train/calibration.py
import numpy as np
import warnings
import threadpoolctl
from sklearn.calibration import CalibratedClassifierCV
from sklearn.isotonic import IsotonicRegression
from sklearn.linear_model import LogisticRegression
from .purged_group_time_series_split import PurgedGroupTimeSeriesSplit
from .thresholds import select_threshold
from .metrics import ece as _ece

# === Wrappers (moved verbatim) ===
class EnsembleCalibratedModel:
    def __init__(self, base_estimator, calibrators):
        self.base_estimator = base_estimator
        self.calibrators = calibrators or []
    def predict_proba(self, X):
        if not self.calibrators:
            return self.base_estimator.predict_proba(X)

thinking
**Analyzing calibration CV setup**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '160,260p' pipeline/train.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 679ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
        )
        active_cutoff_hours = float(get_decision_cutoff_hours())
        cutoff_label = format_cutoff_label(active_cutoff_hours)
        self.cutoff_label = "default"
        default_feature_path = self.paths.feature_matrix_path
        labeled_feature_path = default_feature_path.with_name(
            f"{default_feature_path.stem}_{cutoff_label}{default_feature_path.suffix}"
        )
        if labeled_feature_path.exists():
            self.paths = Paths(
                feature_matrix_path=labeled_feature_path,
                model_dir=self.paths.model_dir,
                metric_dir=self.paths.metric_dir,
                analysis_dir=self.paths.analysis_dir,
            )
            self.config['data']['feature_matrix_path'] = str(labeled_feature_path)
            self.cutoff_label = cutoff_label
            logger.info(
                "Using feature matrix for cutoff %.2f hours (%s) → %s",

thinking
**Designing purged time-series split**

thinking
**Designing time-based purged splits**

I'm thinking about adapting PurgedGroupTimeSeriesSplit to handle purging by days or games using config values like `purge_window_days` or `purge_window_games`. Since it uses a fixed group gap, I’m considering building a helper that applies a time-based purge after generating splits, removing training rows occurring within a purge window before the test period, possibly also applying an embargo after. This way, I can maintain proper temporal separation aligned with the user’s config.

thinking
**Planning time-based data splitting enhancements**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg "group_gap" -n pipeline/train.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 760ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
991:    def _convert_embargo_days_to_group_gap(
997:        """Approximate PurgedGroupTimeSeriesSplit group_gap from day-based embargo config."""
1140:        embargo_days = int(self.config['data_split'].get('group_gap', self.config['data_split'].get('embargo_td', 0) or 0))
1142:        group_gap = self._convert_embargo_days_to_group_gap(df_full, groups_series, embargo_days)
1148:            group_gap=group_gap,
1155:            "Generating purged OOF predictions for %s on %s (n_splits=%d, embargo_days=%d, group_gap=%d)...",
1160:            group_gap,

thinking
**Checking for existing function implementation**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '960,1180p' pipeline/train.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 697ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
        logger.info(f"_apply_feature_artifacts input shape: {df.shape}")
        artifacts = FeatureArtifacts(
            feature_columns=list(self.feature_columns or []),
            datetime_features=self.datetime_features.get(problem_name, []) or [],
            categorical_features=self.categorical_features.get(problem_name, []) or [],
            category_levels=self.category_levels.get(problem_name, {}) or {},
        )
        X, y = transform_with_artifacts(df, artifacts, self.target_col)
        logger.info(f"_apply_feature_artifacts reindex shape: {X.shape}")
        return X, y


    def _build_group_labels(self, df: pd.DataFrame) -> pd.Series:
        """Construct group labels prioritising the configured group_col."""
        if df is None or df.empty:
            return pd.Series(dtype="object")
        if self.group_col and self.group_col in df.columns:
            labels = df[self.group_col].fillna("__missing__").astype(str)
        elif {"season", "week"}.issubset(df.columns):

thinking
**Examining group gap and purge handling**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg "_generate_oof_predictions" -n pipeline/train.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 712ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
1133:    def _generate_oof_predictions(self, model_name, problem_config, X, y, groups_series, sample_weight=None):
1261:                    oof_preds = self._generate_oof_predictions(
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1230,1295p' pipeline/train.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 725ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
        ----------
        problem_config : dict
        X_train_full, y_train_full : pd.DataFrame / pd.Series
            Training data.
        X_test, y_test : pd.DataFrame / pd.Series
            Feature-restricted test set (post-artifact application) and labels.
        df_test_full : pd.DataFrame
            The *raw* test slice straight from the feature parquet; used to
            enrich the correct / incorrect prediction dumps with every column
            available in the pipeline output.
        sample_weight_train_full : pd.Series or None
            Per-row weights aligned with X_train_full (train+validation data).
        """
        model_names = [m for m in self.config['training']['models_to_train']]
        problem_name = problem_config['name']
        self.current_problem_name = problem_name
        # Sample weighting removed
        
        if sample_weight_train_full is not None:

thinking
**Searching for group index assignment**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg "_groups_index_map" -n pipeline/train.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 744ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
275:        self._groups_index_map = {} # Map problem -> Series(index=df_train_full.index, value=group label)
1259:                groups_full_series = self._groups_index_map.get(problem_name)
1590:                        groups_series = self._groups_index_map.get(problem_name)
1890:            groups_series = self._groups_index_map.get(problem_name)
2486:                self._groups_index_map[problem_name] = group_labels_full
2488:                self._groups_index_map[problem_name] = None
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '2440,2515p' pipeline/train.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 706ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
            
            group_labels_train = self._build_group_labels(df_train)
            groups_train = group_labels_train.to_numpy()

            # --- RE-ENABLE HYPERPARAMETER TUNING ---
            if self.cfg.hyperparameter_tuning.get('run_tuning', True):
                for model_name in [m for m in self.cfg.models_to_train]:
                    logger.info(f"--- Tuning hyperparameters for {model_name.upper()} on {problem_name.upper()} ---")
                    self.tune_hyperparameters(
                        model_name,
                        problem_copy,
                        X_train,
                        y_train,
                        groups_train,
                        sample_weight=sample_weight_train,
                    )
            else:
                logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
            # -----------------------------------------

thinking
**Designing time-aware purge and embargo**

thinking
**Planning purged time series splitting**

thinking
**Designing purged group time splits helper**

thinking
**Improving time series split logic**

I'm refining the purge and embargo logic to avoid empty training folds and ensure train data precedes validation periods strictly. I'm also planning tests to verify no group overlap and correct time-based splits and considering integrating a new helper function into the training and tuning modules to handle purged group splits more cleanly.

thinking
**Planning purged cross-validation integration**

thinking
**Planning time-aware purged CV integration**

I'll design a new helper to build time-aware purged splits using the full data frame and update training and tuning functions to use it, ensuring consistent cross-validation with time-based purging; I'll also add tests and consider aligning config fields accordingly.

thinking
**Planning time-aware purged splits helper**

I'm outlining how to update the data split logic to support a purge window parameter, considering compatibility with existing config fields. I want to carefully handle sorting and index alignment between the DataFrame and group labels to ensure the splits correspond correctly to the training data order. Let's keep changes minimal but robust for testing and usage.

thinking
**Enforcing time order assumptions**

thinking
**Revising group splitting with custom logic**

I'm working out how to handle group-based time splits when groups aren't contiguous in time-sorted data, since the existing method requires contiguous groups. Sorting by both time and group might reorder data and require adjusting X/y together. Instead, I’m leaning toward a custom splitter that respects group chronology without contiguity, by ordering groups via their earliest times and mapping indices accordingly. This way, I can keep data aligned and avoid contiguity constraints.

thinking
**Designing group-based time splits**

thinking
**Describing time-aware group splits**

thinking
**Designing time-based group splitting logic**

I'm outlining a function to split data by group and time with purging and embargo windows to avoid leakage, considering how to handle group timing and choosing which groups to include or exclude in training versus testing folds. I'll think through purge and embargo effects carefully to align with known split strategies.

thinking
**Evaluating embargo and purge logic for time-series splits**

thinking
**Centralizing purged CV utilities**

I'm thinking of creating a new helper module in `utils/train/purged_cv.py` to consolidate purged cross-validation functions used by both training and calibration pipelines. This way, I can avoid duplication by updating `_generate_oof_predictions` and `oof_calibrated_probs` to use the shared utilities. I’ll define a `PurgeConfig` dataclass and implement functions like `compute_time_ordered_group_meta` and `generate_purged_group_splits` that work on series inputs and handle tricky cases like non-contiguous groups to ensure no data leakage. Testing will involve simulating grouped time series data to confirm the splits maintain the correct embargo and purge constraints.

thinking
**Planning integration of time-based parameters**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "calibrate_and_select_threshold" -n' in /Users/jameswest/Desktop/nfl_predict succeeded in 767ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
pipeline/train.py:64:    calibrate_and_select_threshold, EnsembleCalibratedModel,
pipeline/train.py:1597:                        final_model, best_thresh, best_method, best_ece = calibrate_and_select_threshold(
pipeline/train.py:1897:            final_model, best_thresh, best_method, best_ece = calibrate_and_select_threshold(
utils/train/calibration.py:140:def calibrate_and_select_threshold(model, X_cal, y_cal, enable, cal_method, thresh_cfg, groups_cal=None, problem_config=None, calibration_cv_folds=3, group_gap=5):
docs/train.md:298:utils.train.calibration.calibrate_and_select_threshold(...)
docs/train.md:334:`utils.train.calibration.calibrate_and_select_threshold(...)`:

thinking
**Updating function calls with new options**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '1570,1615p' pipeline/train.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 713ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
                                "y_cal": np.asarray(y_cal),
                                "y_hat_cal": np.asarray(y_hat_cal_for_store),
                                "meta": (meta_df if meta_df is None else {c: meta_df[c].to_numpy() for c in meta_df.columns}),
                            }
                    except Exception as e:
                        logger.warning(f"[composite] Failed to stash cal preds for {problem_name} (prod path): {e}")
                # --- END NEW ---

                # Classification vs Regression production handling
                if is_classification(problem_config):
                    if is_selective:
                        # For selective models in production, no calibration needed
                        final_model = model
                        best_thresh = None
                        best_method = 'selective'
                        best_ece = None
                    else:
                        # Calibration (use groups when available), respecting config toggle
                        cal_method = self.cfg.calibration_method

thinking
**Planning purged CV helper implementation**

thinking
**Detailing group split logic**

thinking
**Adjusting training group eligibility and purge logic**

I'm refining how to select eligible training groups by ensuring no future data leaks before applying purge windows, handling time zone consistency, and enforcing limits like max group size and group gaps. I’ll integrate these adjustments into the OOF prediction generator, carefully extracting config parameters and making sure data frames include necessary group columns, plus planning fallbacks for edge cases.

thinking
**Planning data split refactor**

I'm figuring out how to modify the hyperparameter tuning to accept the full training DataFrame, so I can reuse a helper to generate purged group splits consistently. This involves creating a new method in ModelTrainer to handle these splits and updating calls to pass the appropriate data without leaking validation information. I'll need to handle cases where group columns might be missing and decide how to fallback safely.

thinking
**Planning hyperparameter tuning updates**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "def tune_hyperparameters_ext" -n utils/train/tuning.py' in /Users/jameswest/Desktop/nfl_predict exited 1 in 762ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted

thinking
**Searching for "tune_hyperparameters_ext"**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "tune_hyperparameters_ext" utils/train/tuning.py' in /Users/jameswest/Desktop/nfl_predict exited 1 in 731ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted

thinking
**Checking for definitions deeper in file**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '220,520p' utils/train/tuning.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 747ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
            window_low = max(low, base_value - half_span)
            window_high = min(high, base_value + half_span)
            if window_low >= window_high:
                window_low, window_high = low, high
        if spec["type"] == "int":
            low_int = int(math.floor(window_low))
            high_int = int(math.ceil(window_high))
            if low_int == high_int:
                high_int = low_int + 1
            return trial.suggest_int(name, low_int, high_int)
        log_sampling = bool(spec.get("log", False))
        return trial.suggest_float(name, float(window_low), float(window_high), log=log_sampling)

    skip_keys = {"grow_policy", "max_depth", "max_leaves"}
    for param_name, spec in param_specs.items():
        if param_name in skip_keys:
            continue
        if spec["kind"] == "categorical":
            params[param_name] = trial.suggest_categorical(param_name, spec["choices"])

thinking
**Considering CV splitter enhancements**

I'm mapping out how to upgrade `_prepare_splits` to handle new purge-aware options by passing additional params or a splitter factory, since `tune_hyperparameters` already gets the trainer instance. I’m weighing if adjusting `_prepare_splits` to accept the trainer or splitter config is feasible, or if focusing just on purge-aware OOF prediction and tests is better given time constraints.

thinking
**Designing time-aware hyperparameter tuning**

I'm figuring out how to pass training data with time information into hyperparameter tuning functions. Since the current `_prepare_splits` lacks direct access to time data, I'll modify `tune_hyperparameters` and its extended version to accept `time_values` extracted from the training DataFrame and update splitting logic to use a purged group time series split with accurate time and group metadata. This approach balances backward compatibility with improved time-aware tuning.

thinking
**Planning purged cross-validation helper**

I’m organizing how to add a helper for purged cross-validation based on config values like purge days and group gaps, aiming to integrate it into prediction generation and hyperparameter tuning flows. I'll add the helper as a new module, update relevant training pipeline files to use it, adjust method signatures to accept extra metadata, and plan tests to verify the functionality—all while keeping config updates minimal and ensuring dependencies like numpy and pandas are properly imported.
file update
A /Users/jameswest/Desktop/nfl_predict/utils/train/purged_cv.py
"""Time-aware, purged group CV helpers to prevent temporal leakage."""

from __future__ import annotations

from typing import List, Tuple

import numpy as np
import pandas as pd


def _determine_test_group_size(
    n_groups: int,
    n_splits: int,
    max_test_group_size: int | float,
    explicit: int | None,
) -> int:
    """Choose a reasonable test group size given constraints."""
    if explicit is not None:
        size = int(explicit)
        if size <= 0:
            raise ValueError("test_group_size must be positive when provided.")
        if size > n_groups:
            raise ValueError("test_group_size cannot exceed number of groups.")
        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)

    denom = max(n_splits + 1, 2)
    size = max(1, n_groups // denom)
    if np.isfinite(max_test_group_size):
        size = min(size, int(max_test_group_size))
    return size


def compute_purged_group_splits(
    df: pd.DataFrame,
    *,
    time_col: str,
    group_col: str,
    n_splits: int,
    purge_days: float = 0.0,
    purge_groups: int = 0,
    embargo_days: float = 0.0,
    max_train_group_size: int | float = np.inf,
    max_test_group_size: int | float = np.inf,
    test_group_size: int | None = None,
) -> List[Tuple[np.ndarray, np.ndarray]]:
    """Build tail-heavy, time-ordered splits with purge/embargo windows.

    Groups are ordered by their earliest timestamp. Train groups come strictly
    before the validation block, then are purged by both a group-gap and a
    time-based window to respect rolling feature lookbacks.
    """

    if time_col not in df.columns:
        raise ValueError(f"time_col '{time_col}' missing from frame.")
    if group_col not in df.columns:
        raise ValueError(f"group_col '{group_col}' missing from frame.")
    if n_splits < 2:
        raise ValueError("n_splits must be at least 2 for CV.")

    meta = df[[time_col, group_col]].copy()
    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
    meta = meta.dropna(subset=[time_col, group_col])

    if meta.empty:
        return []

    meta = meta.sort_values([time_col, group_col]).reset_index()
    meta.rename(columns={"index": "_orig_idx"}, inplace=True)

    group_stats = (
        meta.groupby(group_col)[time_col]
        .agg(["min", "max"])
        .rename(columns={"min": "t_min", "max": "t_max"})
        .sort_values("t_min")
    )
    group_order = group_stats.index.to_list()
    n_groups = len(group_order)
    if n_groups < n_splits + 1:
        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")

    group_to_indices = {
        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
        for grp in group_order
    }

    t_min = group_stats["t_min"]
    t_max = group_stats["t_max"]

    test_size = _determine_test_group_size(
        n_groups=n_groups,
        n_splits=n_splits,
        max_test_group_size=max_test_group_size,
        explicit=test_group_size,
    )
    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))

    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)

    splits: List[Tuple[np.ndarray, np.ndarray]] = []
    for start in test_starts:
        test_groups = group_order[start : start + test_size]
        if not test_groups:
            continue

        earliest_test = t_min.loc[test_groups].min()
        latest_test = t_max.loc[test_groups].max()

        candidate_train = group_order[:start]
        if purge_groups > 0:
            candidate_train = candidate_train[:-int(purge_groups)] or []

        # Enforce time-based purge and embargo (train strictly before test).
        keep_train: list[str] = []
        purge_cutoff = earliest_test - purge_delta
        embargo_cutoff = latest_test + embargo_delta
        for grp in candidate_train:
            if t_max.loc[grp] >= purge_cutoff:
                continue
            if t_max.loc[grp] >= earliest_test:
                continue
            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
                # This branch mainly protects against oddly-ordered frames; with tail splits
                # train groups should already precede the test window.
                continue
            keep_train.append(grp)

        if np.isfinite(max_train_group_size):
            keep_train = keep_train[-int(max_train_group_size) :]

        if not keep_train:
            continue

        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
        if not train_idx_parts or not test_idx_parts:
            continue

        train_idx = np.sort(np.concatenate(train_idx_parts))
        test_idx = np.sort(np.concatenate(test_idx_parts))
        if len(train_idx) == 0 or len(test_idx) == 0:
            continue
        splits.append((train_idx, test_idx))

    return splits

apply_patch(auto_approved=true) exited 0 in 72ms:
Success. Updated the following files:
A utils/train/purged_cv.py
file update:
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+

file update:
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+


thinking
**Updating imports in training pipeline**
file update:
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+


thinking
**Inspecting import section**
exec
/bin/zsh -lc "cd /Users/jameswest/Desktop/nfl_predict && sed -n '40,120p' pipeline/train.py" in /Users/jameswest/Desktop/nfl_predict succeeded in 792ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
import argparse
import copy
from sklearn.calibration import CalibratedClassifierCV
import warnings
import gc

# New utility imports
from pandas.api.types import is_datetime64_any_dtype
from utils.train.model_factory import (
    get_task_type, is_classification, is_regression, get_model_instance, auto_spw
)
from utils.train.xgb_utils import (
    fit_model_es, get_best_iteration, compute_patience, predict_proba_batched, retrain_with_best_iter
)
from utils.train.data import load_feature_matrix
from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
from utils.train.feature_artifacts import (
    FeatureArtifacts,
file update:
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+


thinking
**Adding import for compute_purged_group_splits**
file update
M /Users/jameswest/Desktop/nfl_predict/pipeline/train.py
@@ -55,2 +55,3 @@
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
apply_patch(auto_approved=true) exited 0 in 40ms:
Success. Updated the following files:
M pipeline/train.py
file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..d976adb54727bb59a97bdedcc6ecac17569e38a9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+

file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..d976adb54727bb59a97bdedcc6ecac17569e38a9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+


thinking
**Preparing to update hyperparameter tuning method**
exec
/bin/zsh -lc 'cd /Users/jameswest/Desktop/nfl_predict && rg -n "def tune_hyperparameters" pipeline/train.py' in /Users/jameswest/Desktop/nfl_predict succeeded in 793ms:
/opt/homebrew/Library/Homebrew/cmd/shellenv.sh: line 18: /bin/ps: Operation not permitted
1120:    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..d976adb54727bb59a97bdedcc6ecac17569e38a9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+


thinking
**Updating function signature**
file update
M /Users/jameswest/Desktop/nfl_predict/pipeline/train.py
@@ -1119,3 +1119,12 @@
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
@@ -1129,2 +1138,6 @@
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
apply_patch(auto_approved=true) exited 0 in 54ms:
Success. Updated the following files:
M pipeline/train.py
file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..33ee21d4f9c2876ce9f4b40199e46c050c837f49
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+

file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..33ee21d4f9c2876ce9f4b40199e46c050c837f49
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+


thinking
**Modifying call site to include time values**
file update
M /Users/jameswest/Desktop/nfl_predict/pipeline/train.py
@@ -2468,2 +2468,3 @@
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
apply_patch(auto_approved=true) exited 0 in 39ms:
Success. Updated the following files:
M pipeline/train.py
file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+

file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+


thinking
**Modifying tuning.py function signature and imports**
file update
M /Users/jameswest/Desktop/nfl_predict/utils/train/tuning.py
@@ -10,2 +10,3 @@
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 
apply_patch(auto_approved=true) exited 0 in 144ms:
Success. Updated the following files:
M utils/train/tuning.py
file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+
diff --git a/utils/train/tuning.py b/utils/train/tuning.py
index ee0212fd69d3a84a5ab2b58bc9e43aed0934d167..bc4e5b7c17044b941d9985bc56cd435c4e7b5dac
--- a/utils/train/tuning.py
+++ b/utils/train/tuning.py
@@ -8,6 +8,7 @@
 from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, precision_score
 from sklearn.model_selection import TimeSeriesSplit
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 
 logger = logging.getLogger(__name__)
 

file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+
diff --git a/utils/train/tuning.py b/utils/train/tuning.py
index ee0212fd69d3a84a5ab2b58bc9e43aed0934d167..bc4e5b7c17044b941d9985bc56cd435c4e7b5dac
--- a/utils/train/tuning.py
+++ b/utils/train/tuning.py
@@ -8,6 +8,7 @@
 from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, precision_score
 from sklearn.model_selection import TimeSeriesSplit
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 
 logger = logging.getLogger(__name__)
 


thinking
**Updating _prepare_splits for purge params**

thinking
**Updating time series split parameters**
file update
M /Users/jameswest/Desktop/nfl_predict/utils/train/tuning.py
@@ -171,7 +171,58 @@
 
-def _prepare_splits(X_train, y_train, groups_train):
-    # Try purged CV; fallback to time series
+def _prepare_splits(
+    X_train,
+    y_train,
+    groups_train,
+    *,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size=np.inf,
+    max_test_group_size=np.inf,
+):
+    # Prefer explicit time-aware purge when timestamps are available
+    n_splits = 5
+    if time_values is not None:
+        try:
+            import pandas as pd
+
+            meta_df = pd.DataFrame(
+                {
+                    "time": pd.to_datetime(time_values),
+                    "group": groups_train,
+                }
+            )
+            splits = compute_purged_group_splits(
+                meta_df,
+                time_col="time",
+                group_col="group",
+                n_splits=max(2, int(n_splits)),
+                purge_days=purge_days,
+                purge_groups=int(purge_groups),
+                embargo_days=embargo_days,
+                max_train_group_size=max_train_group_size,
+                max_test_group_size=max_test_group_size,
+            )
+            if splits:
+                logger.info(
+                    "Pre-computing CV splits with time-aware purge (n_splits=%d, purge_days=%.1f, purge_groups=%d).",
+                    n_splits,
+                    float(purge_days),
+                    int(purge_groups),
+                )
+                return splits
+        except Exception as e:  # pragma: no cover - guarded fallback
+            logger.warning(
+                "Time-aware purged CV failed (%s); falling back to group-gap only.",
+                e,
+            )
+
+    # Try group-gap based purged CV; fallback to time series
     try:
-        cv = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)
-        logger.info("Pre-computing CV splits with PurgedGroupTimeSeriesSplit...")
+        cv = PurgedGroupTimeSeriesSplit(n_splits=max(2, int(n_splits)), group_gap=int(purge_groups))
+        logger.info(
+            "Pre-computing CV splits with PurgedGroupTimeSeriesSplit (group_gap=%d)...",
+            int(purge_groups),
+        )
         return list(cv.split(X_train, y_train, groups=groups_train))
apply_patch(auto_approved=true) exited 0 in 55ms:
Success. Updated the following files:
M utils/train/tuning.py
file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+
diff --git a/utils/train/tuning.py b/utils/train/tuning.py
index ee0212fd69d3a84a5ab2b58bc9e43aed0934d167..205470859038317b7daf5e4a2e87e7560a743e4a
--- a/utils/train/tuning.py
+++ b/utils/train/tuning.py
@@ -8,6 +8,7 @@
 from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, precision_score
 from sklearn.model_selection import TimeSeriesSplit
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 
 logger = logging.getLogger(__name__)
 
@@ -168,11 +169,62 @@
         return 1.0
     return float(np.clip((1.0 - p) / p, 0.25, 50.0))
 
-def _prepare_splits(X_train, y_train, groups_train):
-    # Try purged CV; fallback to time series
+def _prepare_splits(
+    X_train,
+    y_train,
+    groups_train,
+    *,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size=np.inf,
+    max_test_group_size=np.inf,
+):
+    # Prefer explicit time-aware purge when timestamps are available
+    n_splits = 5
+    if time_values is not None:
+        try:
+            import pandas as pd
+
+            meta_df = pd.DataFrame(
+                {
+                    "time": pd.to_datetime(time_values),
+                    "group": groups_train,
+                }
+            )
+            splits = compute_purged_group_splits(
+                meta_df,
+                time_col="time",
+                group_col="group",
+                n_splits=max(2, int(n_splits)),
+                purge_days=purge_days,
+                purge_groups=int(purge_groups),
+                embargo_days=embargo_days,
+                max_train_group_size=max_train_group_size,
+                max_test_group_size=max_test_group_size,
+            )
+            if splits:
+                logger.info(
+                    "Pre-computing CV splits with time-aware purge (n_splits=%d, purge_days=%.1f, purge_groups=%d).",
+                    n_splits,
+                    float(purge_days),
+                    int(purge_groups),
+                )
+                return splits
+        except Exception as e:  # pragma: no cover - guarded fallback
+            logger.warning(
+                "Time-aware purged CV failed (%s); falling back to group-gap only.",
+                e,
+            )
+
+    # Try group-gap based purged CV; fallback to time series
     try:
-        cv = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)
-        logger.info("Pre-computing CV splits with PurgedGroupTimeSeriesSplit...")
+        cv = PurgedGroupTimeSeriesSplit(n_splits=max(2, int(n_splits)), group_gap=int(purge_groups))
+        logger.info(
+            "Pre-computing CV splits with PurgedGroupTimeSeriesSplit (group_gap=%d)...",
+            int(purge_groups),
+        )
         return list(cv.split(X_train, y_train, groups=groups_train))
     except ValueError as e:
         logger.warning(f"PurgedGroupTimeSeriesSplit failed with error: {e}. Falling back to standard TimeSeriesSplit.")

file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+
diff --git a/utils/train/tuning.py b/utils/train/tuning.py
index ee0212fd69d3a84a5ab2b58bc9e43aed0934d167..205470859038317b7daf5e4a2e87e7560a743e4a
--- a/utils/train/tuning.py
+++ b/utils/train/tuning.py
@@ -8,6 +8,7 @@
 from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, precision_score
 from sklearn.model_selection import TimeSeriesSplit
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 
 logger = logging.getLogger(__name__)
 
@@ -168,11 +169,62 @@
         return 1.0
     return float(np.clip((1.0 - p) / p, 0.25, 50.0))
 
-def _prepare_splits(X_train, y_train, groups_train):
-    # Try purged CV; fallback to time series
+def _prepare_splits(
+    X_train,
+    y_train,
+    groups_train,
+    *,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size=np.inf,
+    max_test_group_size=np.inf,
+):
+    # Prefer explicit time-aware purge when timestamps are available
+    n_splits = 5
+    if time_values is not None:
+        try:
+            import pandas as pd
+
+            meta_df = pd.DataFrame(
+                {
+                    "time": pd.to_datetime(time_values),
+                    "group": groups_train,
+                }
+            )
+            splits = compute_purged_group_splits(
+                meta_df,
+                time_col="time",
+                group_col="group",
+                n_splits=max(2, int(n_splits)),
+                purge_days=purge_days,
+                purge_groups=int(purge_groups),
+                embargo_days=embargo_days,
+                max_train_group_size=max_train_group_size,
+                max_test_group_size=max_test_group_size,
+            )
+            if splits:
+                logger.info(
+                    "Pre-computing CV splits with time-aware purge (n_splits=%d, purge_days=%.1f, purge_groups=%d).",
+                    n_splits,
+                    float(purge_days),
+                    int(purge_groups),
+                )
+                return splits
+        except Exception as e:  # pragma: no cover - guarded fallback
+            logger.warning(
+                "Time-aware purged CV failed (%s); falling back to group-gap only.",
+                e,
+            )
+
+    # Try group-gap based purged CV; fallback to time series
     try:
-        cv = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)
-        logger.info("Pre-computing CV splits with PurgedGroupTimeSeriesSplit...")
+        cv = PurgedGroupTimeSeriesSplit(n_splits=max(2, int(n_splits)), group_gap=int(purge_groups))
+        logger.info(
+            "Pre-computing CV splits with PurgedGroupTimeSeriesSplit (group_gap=%d)...",
+            int(purge_groups),
+        )
         return list(cv.split(X_train, y_train, groups=groups_train))
     except ValueError as e:
         logger.warning(f"PurgedGroupTimeSeriesSplit failed with error: {e}. Falling back to standard TimeSeriesSplit.")


thinking
**Updating function signature for tuning hyperparameters**
file update
M /Users/jameswest/Desktop/nfl_predict/utils/train/tuning.py
@@ -326,3 +326,15 @@
 # ---- public API ----
-def tune_hyperparameters(trainer, model_name: str, problem_config: Dict, X_train, y_train, groups_train, sample_weight=None):
+def tune_hyperparameters(
+    trainer,
+    model_name: str,
+    problem_config: Dict,
+    X_train,
+    y_train,
+    groups_train,
+    sample_weight=None,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+):
     """Drop-in replacement for ModelTrainer.tune_hyperparameters; writes into trainer.best_params & meta."""
apply_patch(auto_approved=true) exited 0 in 64ms:
Success. Updated the following files:
M utils/train/tuning.py
file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+
diff --git a/utils/train/tuning.py b/utils/train/tuning.py
index ee0212fd69d3a84a5ab2b58bc9e43aed0934d167..57ac7682ebf9e247d7d6c9e607494ab528f7f89d
--- a/utils/train/tuning.py
+++ b/utils/train/tuning.py
@@ -8,6 +8,7 @@
 from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, precision_score
 from sklearn.model_selection import TimeSeriesSplit
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 
 logger = logging.getLogger(__name__)
 
@@ -168,11 +169,62 @@
         return 1.0
     return float(np.clip((1.0 - p) / p, 0.25, 50.0))
 
-def _prepare_splits(X_train, y_train, groups_train):
-    # Try purged CV; fallback to time series
+def _prepare_splits(
+    X_train,
+    y_train,
+    groups_train,
+    *,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size=np.inf,
+    max_test_group_size=np.inf,
+):
+    # Prefer explicit time-aware purge when timestamps are available
+    n_splits = 5
+    if time_values is not None:
+        try:
+            import pandas as pd
+
+            meta_df = pd.DataFrame(
+                {
+                    "time": pd.to_datetime(time_values),
+                    "group": groups_train,
+                }
+            )
+            splits = compute_purged_group_splits(
+                meta_df,
+                time_col="time",
+                group_col="group",
+                n_splits=max(2, int(n_splits)),
+                purge_days=purge_days,
+                purge_groups=int(purge_groups),
+                embargo_days=embargo_days,
+                max_train_group_size=max_train_group_size,
+                max_test_group_size=max_test_group_size,
+            )
+            if splits:
+                logger.info(
+                    "Pre-computing CV splits with time-aware purge (n_splits=%d, purge_days=%.1f, purge_groups=%d).",
+                    n_splits,
+                    float(purge_days),
+                    int(purge_groups),
+                )
+                return splits
+        except Exception as e:  # pragma: no cover - guarded fallback
+            logger.warning(
+                "Time-aware purged CV failed (%s); falling back to group-gap only.",
+                e,
+            )
+
+    # Try group-gap based purged CV; fallback to time series
     try:
-        cv = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)
-        logger.info("Pre-computing CV splits with PurgedGroupTimeSeriesSplit...")
+        cv = PurgedGroupTimeSeriesSplit(n_splits=max(2, int(n_splits)), group_gap=int(purge_groups))
+        logger.info(
+            "Pre-computing CV splits with PurgedGroupTimeSeriesSplit (group_gap=%d)...",
+            int(purge_groups),
+        )
         return list(cv.split(X_train, y_train, groups=groups_train))
     except ValueError as e:
         logger.warning(f"PurgedGroupTimeSeriesSplit failed with error: {e}. Falling back to standard TimeSeriesSplit.")
@@ -272,7 +324,19 @@
 
 
 # ---- public API ----
-def tune_hyperparameters(trainer, model_name: str, problem_config: Dict, X_train, y_train, groups_train, sample_weight=None):
+def tune_hyperparameters(
+    trainer,
+    model_name: str,
+    problem_config: Dict,
+    X_train,
+    y_train,
+    groups_train,
+    sample_weight=None,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+):
     """Drop-in replacement for ModelTrainer.tune_hyperparameters; writes into trainer.best_params & meta."""
     task_type = problem_config.get("task_type", "classification").lower()
     direction = "minimize" if task_type == "regression" else "maximize"

file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+
diff --git a/utils/train/tuning.py b/utils/train/tuning.py
index ee0212fd69d3a84a5ab2b58bc9e43aed0934d167..57ac7682ebf9e247d7d6c9e607494ab528f7f89d
--- a/utils/train/tuning.py
+++ b/utils/train/tuning.py
@@ -8,6 +8,7 @@
 from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, precision_score
 from sklearn.model_selection import TimeSeriesSplit
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 
 logger = logging.getLogger(__name__)
 
@@ -168,11 +169,62 @@
         return 1.0
     return float(np.clip((1.0 - p) / p, 0.25, 50.0))
 
-def _prepare_splits(X_train, y_train, groups_train):
-    # Try purged CV; fallback to time series
+def _prepare_splits(
+    X_train,
+    y_train,
+    groups_train,
+    *,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size=np.inf,
+    max_test_group_size=np.inf,
+):
+    # Prefer explicit time-aware purge when timestamps are available
+    n_splits = 5
+    if time_values is not None:
+        try:
+            import pandas as pd
+
+            meta_df = pd.DataFrame(
+                {
+                    "time": pd.to_datetime(time_values),
+                    "group": groups_train,
+                }
+            )
+            splits = compute_purged_group_splits(
+                meta_df,
+                time_col="time",
+                group_col="group",
+                n_splits=max(2, int(n_splits)),
+                purge_days=purge_days,
+                purge_groups=int(purge_groups),
+                embargo_days=embargo_days,
+                max_train_group_size=max_train_group_size,
+                max_test_group_size=max_test_group_size,
+            )
+            if splits:
+                logger.info(
+                    "Pre-computing CV splits with time-aware purge (n_splits=%d, purge_days=%.1f, purge_groups=%d).",
+                    n_splits,
+                    float(purge_days),
+                    int(purge_groups),
+                )
+                return splits
+        except Exception as e:  # pragma: no cover - guarded fallback
+            logger.warning(
+                "Time-aware purged CV failed (%s); falling back to group-gap only.",
+                e,
+            )
+
+    # Try group-gap based purged CV; fallback to time series
     try:
-        cv = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)
-        logger.info("Pre-computing CV splits with PurgedGroupTimeSeriesSplit...")
+        cv = PurgedGroupTimeSeriesSplit(n_splits=max(2, int(n_splits)), group_gap=int(purge_groups))
+        logger.info(
+            "Pre-computing CV splits with PurgedGroupTimeSeriesSplit (group_gap=%d)...",
+            int(purge_groups),
+        )
         return list(cv.split(X_train, y_train, groups=groups_train))
     except ValueError as e:
         logger.warning(f"PurgedGroupTimeSeriesSplit failed with error: {e}. Falling back to standard TimeSeriesSplit.")
@@ -272,7 +324,19 @@
 
 
 # ---- public API ----
-def tune_hyperparameters(trainer, model_name: str, problem_config: Dict, X_train, y_train, groups_train, sample_weight=None):
+def tune_hyperparameters(
+    trainer,
+    model_name: str,
+    problem_config: Dict,
+    X_train,
+    y_train,
+    groups_train,
+    sample_weight=None,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+):
     """Drop-in replacement for ModelTrainer.tune_hyperparameters; writes into trainer.best_params & meta."""
     task_type = problem_config.get("task_type", "classification").lower()
     direction = "minimize" if task_type == "regression" else "maximize"


thinking
**Updating _prepare_splits call**
file update
M /Users/jameswest/Desktop/nfl_predict/utils/train/tuning.py
@@ -367,3 +367,13 @@
         logger.info(f"Pre-computing CV splits for {problem_name}...")
-        splits = _prepare_splits(X_train, y_train, groups_train)
+        splits = _prepare_splits(
+            X_train,
+            y_train,
+            groups_train,
+            time_values=time_values,
+            purge_days=purge_days,
+            purge_groups=purge_groups,
+            embargo_days=embargo_days,
+            max_train_group_size=problem_config.get("max_train_group_size", np.inf),
+            max_test_group_size=problem_config.get("max_test_group_size", np.inf),
+        )
         trainer._cached_splits[cache_key] = splits
apply_patch(auto_approved=true) exited 0 in 51ms:
Success. Updated the following files:
M utils/train/tuning.py
file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+
diff --git a/utils/train/tuning.py b/utils/train/tuning.py
index ee0212fd69d3a84a5ab2b58bc9e43aed0934d167..734f05bda32024edbea23bd0d05ec292c1fd3904
--- a/utils/train/tuning.py
+++ b/utils/train/tuning.py
@@ -8,6 +8,7 @@
 from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, precision_score
 from sklearn.model_selection import TimeSeriesSplit
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 
 logger = logging.getLogger(__name__)
 
@@ -168,11 +169,62 @@
         return 1.0
     return float(np.clip((1.0 - p) / p, 0.25, 50.0))
 
-def _prepare_splits(X_train, y_train, groups_train):
-    # Try purged CV; fallback to time series
+def _prepare_splits(
+    X_train,
+    y_train,
+    groups_train,
+    *,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size=np.inf,
+    max_test_group_size=np.inf,
+):
+    # Prefer explicit time-aware purge when timestamps are available
+    n_splits = 5
+    if time_values is not None:
+        try:
+            import pandas as pd
+
+            meta_df = pd.DataFrame(
+                {
+                    "time": pd.to_datetime(time_values),
+                    "group": groups_train,
+                }
+            )
+            splits = compute_purged_group_splits(
+                meta_df,
+                time_col="time",
+                group_col="group",
+                n_splits=max(2, int(n_splits)),
+                purge_days=purge_days,
+                purge_groups=int(purge_groups),
+                embargo_days=embargo_days,
+                max_train_group_size=max_train_group_size,
+                max_test_group_size=max_test_group_size,
+            )
+            if splits:
+                logger.info(
+                    "Pre-computing CV splits with time-aware purge (n_splits=%d, purge_days=%.1f, purge_groups=%d).",
+                    n_splits,
+                    float(purge_days),
+                    int(purge_groups),
+                )
+                return splits
+        except Exception as e:  # pragma: no cover - guarded fallback
+            logger.warning(
+                "Time-aware purged CV failed (%s); falling back to group-gap only.",
+                e,
+            )
+
+    # Try group-gap based purged CV; fallback to time series
     try:
-        cv = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)
-        logger.info("Pre-computing CV splits with PurgedGroupTimeSeriesSplit...")
+        cv = PurgedGroupTimeSeriesSplit(n_splits=max(2, int(n_splits)), group_gap=int(purge_groups))
+        logger.info(
+            "Pre-computing CV splits with PurgedGroupTimeSeriesSplit (group_gap=%d)...",
+            int(purge_groups),
+        )
         return list(cv.split(X_train, y_train, groups=groups_train))
     except ValueError as e:
         logger.warning(f"PurgedGroupTimeSeriesSplit failed with error: {e}. Falling back to standard TimeSeriesSplit.")
@@ -272,7 +324,19 @@
 
 
 # ---- public API ----
-def tune_hyperparameters(trainer, model_name: str, problem_config: Dict, X_train, y_train, groups_train, sample_weight=None):
+def tune_hyperparameters(
+    trainer,
+    model_name: str,
+    problem_config: Dict,
+    X_train,
+    y_train,
+    groups_train,
+    sample_weight=None,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+):
     """Drop-in replacement for ModelTrainer.tune_hyperparameters; writes into trainer.best_params & meta."""
     task_type = problem_config.get("task_type", "classification").lower()
     direction = "minimize" if task_type == "regression" else "maximize"
@@ -301,7 +365,17 @@
         logger.info(f"Re-using cached CV splits for {problem_name}.")
     else:
         logger.info(f"Pre-computing CV splits for {problem_name}...")
-        splits = _prepare_splits(X_train, y_train, groups_train)
+        splits = _prepare_splits(
+            X_train,
+            y_train,
+            groups_train,
+            time_values=time_values,
+            purge_days=purge_days,
+            purge_groups=purge_groups,
+            embargo_days=embargo_days,
+            max_train_group_size=problem_config.get("max_train_group_size", np.inf),
+            max_test_group_size=problem_config.get("max_test_group_size", np.inf),
+        )
         trainer._cached_splits[cache_key] = splits
         logger.info(f"CV splits cached ({len(splits)} splits).")
 

file update:
diff --git a/pipeline/train.py b/pipeline/train.py
index 7c220514d006a3d4815d65a8af9ec7551ccf587e..1d9725f278bd364ba06c82fe5da5da31d3760dd9
--- a/pipeline/train.py
+++ b/pipeline/train.py
@@ -53,6 +53,7 @@
 )
 from utils.train.data import load_feature_matrix
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 from utils.train.sample_weights import compute_sample_weights as compute_sample_weights_util
 from utils.train.feature_artifacts import (
     FeatureArtifacts,
@@ -1116,7 +1117,16 @@
 
 
 
-    def tune_hyperparameters(self, model_name, problem_config, X_train, y_train, groups_train, sample_weight=None):
+    def tune_hyperparameters(
+        self,
+        model_name,
+        problem_config,
+        X_train,
+        y_train,
+        groups_train,
+        sample_weight=None,
+        time_values=None,
+    ):
         """Hyperparameter optimisation via Optuna (currently classification-only). Skips regression tasks."""
         return tune_hyperparameters_ext(
             self,
@@ -1126,6 +1136,10 @@
             y_train,
             groups_train,
             sample_weight=sample_weight,
+            time_values=time_values,
+            purge_days=float(self.config['data_split'].get('purge_td', 0) or self.config['data_split'].get('purge_window_days', 0) or 0),
+            purge_groups=int(self.config['data_split'].get('purge_games', self.config['data_split'].get('group_gap', 0) or 0)),
+            embargo_days=float(self.config['data_split'].get('embargo_td', 0) or 0),
         )
 
 
@@ -2452,6 +2466,7 @@
                         y_train,
                         groups_train,
                         sample_weight=sample_weight_train,
+                        time_values=df_train[self.time_col] if self.time_col in df_train.columns else None,
                     )
             else:
                 logger.info("Skipping hyperparameter tuning as per config `run_tuning: false`.")
diff --git a/utils/train/purged_cv.py b/utils/train/purged_cv.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3753459c7f7c51047df4fc3101112604036bab7
--- /dev/null
+++ b/utils/train/purged_cv.py
@@ -0,0 +1,146 @@
+"""Time-aware, purged group CV helpers to prevent temporal leakage."""
+
+from __future__ import annotations
+
+from typing import List, Tuple
+
+import numpy as np
+import pandas as pd
+
+
+def _determine_test_group_size(
+    n_groups: int,
+    n_splits: int,
+    max_test_group_size: int | float,
+    explicit: int | None,
+) -> int:
+    """Choose a reasonable test group size given constraints."""
+    if explicit is not None:
+        size = int(explicit)
+        if size <= 0:
+            raise ValueError("test_group_size must be positive when provided.")
+        if size > n_groups:
+            raise ValueError("test_group_size cannot exceed number of groups.")
+        return min(size, int(max_test_group_size) if np.isfinite(max_test_group_size) else size)
+
+    denom = max(n_splits + 1, 2)
+    size = max(1, n_groups // denom)
+    if np.isfinite(max_test_group_size):
+        size = min(size, int(max_test_group_size))
+    return size
+
+
+def compute_purged_group_splits(
+    df: pd.DataFrame,
+    *,
+    time_col: str,
+    group_col: str,
+    n_splits: int,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size: int | float = np.inf,
+    max_test_group_size: int | float = np.inf,
+    test_group_size: int | None = None,
+) -> List[Tuple[np.ndarray, np.ndarray]]:
+    """Build tail-heavy, time-ordered splits with purge/embargo windows.
+
+    Groups are ordered by their earliest timestamp. Train groups come strictly
+    before the validation block, then are purged by both a group-gap and a
+    time-based window to respect rolling feature lookbacks.
+    """
+
+    if time_col not in df.columns:
+        raise ValueError(f"time_col '{time_col}' missing from frame.")
+    if group_col not in df.columns:
+        raise ValueError(f"group_col '{group_col}' missing from frame.")
+    if n_splits < 2:
+        raise ValueError("n_splits must be at least 2 for CV.")
+
+    meta = df[[time_col, group_col]].copy()
+    meta[time_col] = pd.to_datetime(meta[time_col], errors="coerce")
+    meta = meta.dropna(subset=[time_col, group_col])
+
+    if meta.empty:
+        return []
+
+    meta = meta.sort_values([time_col, group_col]).reset_index()
+    meta.rename(columns={"index": "_orig_idx"}, inplace=True)
+
+    group_stats = (
+        meta.groupby(group_col)[time_col]
+        .agg(["min", "max"])
+        .rename(columns={"min": "t_min", "max": "t_max"})
+        .sort_values("t_min")
+    )
+    group_order = group_stats.index.to_list()
+    n_groups = len(group_order)
+    if n_groups < n_splits + 1:
+        raise ValueError(f"Insufficient distinct groups ({n_groups}) for n_splits={n_splits}.")
+
+    group_to_indices = {
+        grp: meta.loc[meta[group_col] == grp, "_orig_idx"].to_numpy()
+        for grp in group_order
+    }
+
+    t_min = group_stats["t_min"]
+    t_max = group_stats["t_max"]
+
+    test_size = _determine_test_group_size(
+        n_groups=n_groups,
+        n_splits=n_splits,
+        max_test_group_size=max_test_group_size,
+        explicit=test_group_size,
+    )
+    test_starts = list(range(n_groups - n_splits * test_size, n_groups, test_size))
+
+    purge_delta = pd.Timedelta(days=float(purge_days)) if purge_days else pd.Timedelta(0)
+    embargo_delta = pd.Timedelta(days=float(embargo_days)) if embargo_days else pd.Timedelta(0)
+
+    splits: List[Tuple[np.ndarray, np.ndarray]] = []
+    for start in test_starts:
+        test_groups = group_order[start : start + test_size]
+        if not test_groups:
+            continue
+
+        earliest_test = t_min.loc[test_groups].min()
+        latest_test = t_max.loc[test_groups].max()
+
+        candidate_train = group_order[:start]
+        if purge_groups > 0:
+            candidate_train = candidate_train[:-int(purge_groups)] or []
+
+        # Enforce time-based purge and embargo (train strictly before test).
+        keep_train: list[str] = []
+        purge_cutoff = earliest_test - purge_delta
+        embargo_cutoff = latest_test + embargo_delta
+        for grp in candidate_train:
+            if t_max.loc[grp] >= purge_cutoff:
+                continue
+            if t_max.loc[grp] >= earliest_test:
+                continue
+            if embargo_delta and t_min.loc[grp] <= embargo_cutoff:
+                # This branch mainly protects against oddly-ordered frames; with tail splits
+                # train groups should already precede the test window.
+                continue
+            keep_train.append(grp)
+
+        if np.isfinite(max_train_group_size):
+            keep_train = keep_train[-int(max_train_group_size) :]
+
+        if not keep_train:
+            continue
+
+        train_idx_parts = [group_to_indices[g] for g in keep_train if g in group_to_indices]
+        test_idx_parts = [group_to_indices[g] for g in test_groups if g in group_to_indices]
+        if not train_idx_parts or not test_idx_parts:
+            continue
+
+        train_idx = np.sort(np.concatenate(train_idx_parts))
+        test_idx = np.sort(np.concatenate(test_idx_parts))
+        if len(train_idx) == 0 or len(test_idx) == 0:
+            continue
+        splits.append((train_idx, test_idx))
+
+    return splits
+
diff --git a/utils/train/tuning.py b/utils/train/tuning.py
index ee0212fd69d3a84a5ab2b58bc9e43aed0934d167..734f05bda32024edbea23bd0d05ec292c1fd3904
--- a/utils/train/tuning.py
+++ b/utils/train/tuning.py
@@ -8,6 +8,7 @@
 from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, precision_score
 from sklearn.model_selection import TimeSeriesSplit
 from utils.train.purged_group_time_series_split import PurgedGroupTimeSeriesSplit
+from utils.train.purged_cv import compute_purged_group_splits
 
 logger = logging.getLogger(__name__)
 
@@ -168,11 +169,62 @@
         return 1.0
     return float(np.clip((1.0 - p) / p, 0.25, 50.0))
 
-def _prepare_splits(X_train, y_train, groups_train):
-    # Try purged CV; fallback to time series
+def _prepare_splits(
+    X_train,
+    y_train,
+    groups_train,
+    *,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+    max_train_group_size=np.inf,
+    max_test_group_size=np.inf,
+):
+    # Prefer explicit time-aware purge when timestamps are available
+    n_splits = 5
+    if time_values is not None:
+        try:
+            import pandas as pd
+
+            meta_df = pd.DataFrame(
+                {
+                    "time": pd.to_datetime(time_values),
+                    "group": groups_train,
+                }
+            )
+            splits = compute_purged_group_splits(
+                meta_df,
+                time_col="time",
+                group_col="group",
+                n_splits=max(2, int(n_splits)),
+                purge_days=purge_days,
+                purge_groups=int(purge_groups),
+                embargo_days=embargo_days,
+                max_train_group_size=max_train_group_size,
+                max_test_group_size=max_test_group_size,
+            )
+            if splits:
+                logger.info(
+                    "Pre-computing CV splits with time-aware purge (n_splits=%d, purge_days=%.1f, purge_groups=%d).",
+                    n_splits,
+                    float(purge_days),
+                    int(purge_groups),
+                )
+                return splits
+        except Exception as e:  # pragma: no cover - guarded fallback
+            logger.warning(
+                "Time-aware purged CV failed (%s); falling back to group-gap only.",
+                e,
+            )
+
+    # Try group-gap based purged CV; fallback to time series
     try:
-        cv = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)
-        logger.info("Pre-computing CV splits with PurgedGroupTimeSeriesSplit...")
+        cv = PurgedGroupTimeSeriesSplit(n_splits=max(2, int(n_splits)), group_gap=int(purge_groups))
+        logger.info(
+            "Pre-computing CV splits with PurgedGroupTimeSeriesSplit (group_gap=%d)...",
+            int(purge_groups),
+        )
         return list(cv.split(X_train, y_train, groups=groups_train))
     except ValueError as e:
         logger.warning(f"PurgedGroupTimeSeriesSplit failed with error: {e}. Falling back to standard TimeSeriesSplit.")
@@ -272,7 +324,19 @@
 
 
 # ---- public API ----
-def tune_hyperparameters(trainer, model_name: str, problem_config: Dict, X_train, y_train, groups_train, sample_weight=None):
+def tune_hyperparameters(
+    trainer,
+    model_name: str,
+    problem_config: Dict,
+    X_train,
+    y_train,
+    groups_train,
+    sample_weight=None,
+    time_values=None,
+    purge_days: float = 0.0,
+    purge_groups: int = 0,
+    embargo_days: float = 0.0,
+):
     """Drop-in replacement for ModelTrainer.tune_hyperparameters; writes into trainer.best_params & meta."""
     task_type = problem_config.get("task_type", "classification").lower()
     direction = "minimize" if task_type == "regression" else "maximize"
@@ -301,7 +365,17 @@
         logger.info(f"Re-using cached CV splits for {problem_name}.")
     else:
         logger.info(f"Pre-computing CV splits for {problem_name}...")
-        splits = _prepare_splits(X_train, y_train, groups_train)
+        splits = _prepare_splits(
+            X_train,
+            y_train,
+            groups_train,
+            time_values=time_values,
+            purge_days=purge_days,
+            purge_groups=purge_groups,
+            embargo_days=embargo_days,
+            max_train_group_size=problem_config.get("max_train_group_size", np.inf),
+            max_test_group_size=problem_config.get("max_test_group_size", np.inf),
+        )
         trainer._cached_splits[cache_key] = splits
         logger.info(f"CV splits cached ({len(splits)} splits).")
 

ERROR: You've hit your usage limit. Upgrade to Pro (https://openai.com/chatgpt/pricing), visit https://chatgpt.com/codex/settings/usage to purchase more credits or try again at 10:39 AM.
Warning: no last agent message; wrote empty content to /Users/jameswest/Desktop/nfl_predict/.codex_meta_logs/20251126_084420_phase3or4orval_85951_last_message.txt
tokens used
185,651
