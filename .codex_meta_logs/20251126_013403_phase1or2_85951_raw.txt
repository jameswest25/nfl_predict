OpenAI Codex v0.63.0 (research preview)
--------
workdir: /Users/jameswest/Desktop/nfl_predict
model: gpt-5.1
provider: openai
approval: never
sandbox: read-only
reasoning effort: medium
reasoning summaries: auto
session id: 019abf83-49c5-7250-8ee4-6e49b924f805
--------
user

    Okay, great work.

    Please come up with the richest and most detailed plan possible to address every one of the points
    you brought up in the best way possible.

    Here is your last analysis / set of findings:

    <BEGIN_ANALYSIS_START>
    Here’s a concise pass through the main issues, organized to hit your 7 questions. I’ll call out concrete locations and how they affect realism / predictive performance.

---

**1–2. Conceptual mismatches vs “football reality”**

- **Pre‑snap (`ps_`) stats: training vs inference semantics don’t match reality**
  - Training feature matrix (`pipeline/feature.py` → `utils/feature/player_game_level.py`) computes `ps_*` columns (route participation, slot vs wide targets, scripted touches, etc.) from **actual plays in that same game**.
  - In inference, `_compute_features` in `pipeline/predict.py` does not have those future `ps_*` stats; instead it:
    - Builds rolling features via `add_rolling_features(...)` (`1g_ps_route_participation_pct_per_game`, etc.).
    - Renames them to the **same column names** (`ps_route_participation_pct`, `ps_targets_total`, etc.) via the `rename_map` in `pipeline/predict.py:551–606`.
    - Uses `_apply_ps_fallback` / `_load_ps_baselines` to carry forward last known values for players with no recent tracking (`pipeline/predict.py:520–640`).
  - Training config uses these `ps_*` columns directly as features for multiple problems:
    - `pre_snap_routes` and `pre_snap_scripted_touches` use `ps_route_participation_`, `ps_targets_`, `ps_scripted_` prefixes (`config/training.yaml`).
    - Downstream usage and TD models also include `ps_*` prefixes in `feature_prefixes_to_include`.
  - Effect:
    - In **training**, the model sees “perfect” pre‑snap script and alignment for the *current* game.
    - In **inference**, those exact columns are proxies constructed from **history** and baselines, not the actual script for the upcoming game.
    - This is a big covariate shift and makes the pre‑snap models unrealistically strong in training (they’re partially learning from same‑game play outcomes), then much weaker and noisier at inference.
    - To be realistic, the models that are supposed to approximate pre‑snap plan should be trained on the same kind of information you have at decision time (rolling historical `ps_*`, not the current game’s `ps_*`).

- **Inference for intermediate classification models uses class labels, not probabilities**
  - In `pipeline/predict.py`, `_predict_for_problem` does:
    - `if "anytime_td" in problem_name: proba = model.predict_proba(X)[:, 1]`
    - `else: proba = model.predict(X)` (`pipeline/predict.py:2286–2310`).
  - `availability_active` is a **classification** problem but is treated as regression-ish: `predict()` returns class labels (0/1) for XGBoost classifiers, not calibrated probabilities.
  - Later, you clip these outputs and treat them as probabilities and multiply them into `expected_targets`, `expected_carries`, etc. (`_inject_composed_features` in `pipeline/predict.py:358–454`).
  - Training, however, calibrates and evaluates models using true probabilities (via `calibration.py` and `save_model_and_metrics`).
  - Effect:
    - Availability / snapshare / maybe other classification submodels are effectively **hard‑thresholded** at inference.
    - The whole chain `availability → team pace → usage → efficiency → anytime_td` is then using a much cruder, miscalibrated signal than what training expected.
    - That directly hurts realism (players go from 0 to 1 instead of smooth probabilities) and degrades AUC / calibration for all downstream targets.

- **Pre‑snap “scripted touches” heuristic is strong but somewhat rigid**
  - `_compute_pre_snap_usage` in `utils/feature/player_game_level.py:120–360` defines scripted plays as the first 15 offensive plays (`offense_play_rank < 15`).
  - That’s a plausible heuristic but:
    - It doesn’t adjust for tempo, two‑minute situations, or long drives that blur into non-scripted territory.
    - It assumes every team scripts exactly 15 plays in a uniform way.
  - Effect:
    - For some teams/games, the `ps_scripted_*` features represent a mix of true “openers” and reactive sequences, so model may learn noisy relationships.
    - This matters because `pre_snap_scripted_touches` and downstream TD models treat these as key levers for designed scoring touches.

---

**3. Incomplete implementations / half‑finished paths**

- **Duplicated `efficiency_tds` problem definition in `config/training.yaml`**
  - Around `config/training.yaml:360–430` there are **two** problem blocks with `name: 'efficiency_tds'`:
    - First block includes `drive_hist_`, `weather_`, `travel_`, etc. and a full `other_features_to_include`.
    - Immediately after, a second `efficiency_tds` block appears with a shorter feature set and the same name and `columns_to_discard`.
  - `ModelTrainer` (`pipeline/train.py:2148+`) iterates through `self.problems` as-is, so both entries are used.
  - Effect:
    - You train the same‑named problem twice with slightly different feature configs. Saved artifacts/metrics for that name are whichever run wins last.
    - This is classic config cruft from refactoring; it makes it harder to reason about what the “true” TD efficiency model actually is, and could hide regressions.

- **Pre‑snap models rely on features that don’t exist at inference**
  - `pre_snap_routes` / `pre_snap_scripted_touches` rely on `ps_*` features (current‑game pre‑snap metrics) at training.
  - In inference, `_compute_features` never generates same‑game `ps_*` — it only builds rolling historical approximations and renames them.
  - Effect:
    - The design of pre‑snap models is internally inconsistent: they’re trained with richer, more label‑adjacent features than they will ever see at inference.
    - Prediction pipeline then tries to compensate with `_load_ps_baselines` and `_apply_ps_fallback` (`pipeline/predict.py:520–640`), but that is bolted on rather than built into the training story.

- **LightGBM config is present but effectively unused**
  - `config/training.yaml` defines `models: [lightgbm, xgboost]` and a `lightgbm` hyperparameter block.
  - The actual trainer uses `training.models_to_train` which is set to `['xgboost']` (see `config/training.yaml:470+` and `pipeline/train.py::__init__`).
  - Effect:
    - The LightGBM config is dead weight. It may confuse readers into thinking LightGBM is part of the active ensemble when it isn’t.

---

**4. Wrong / misleading implementations (names vs behavior)**

- **Pre‑snap features reused as “history” but named like contemporaneous stats**
  - In training features, `ps_route_participation_pct` / `ps_targets_*` refer to stats **from that game**.
  - In prediction, the same names are aliased from rolling historical per‑game features (`1g_ps_*_per_game → ps_*`, `3g_ps_*_per_game → ps_*_l3` in `pipeline/predict.py:551–606`).
  - So the name `ps_route_participation_pct` means “actual participation this game” in training but “most recent rolling average” at inference.
  - Effect:
    - The same columns have different semantics between train and predict, which is exactly the opposite of the project’s goal (“training featurization must conceptually and logically match prediction featurization”).
    - This will produce subtle but real degradation in calibration and generalization for any problem that leans on `ps_*` features.

- **Prediction code assumes only `anytime_td` is classification**
  - `_predict_for_problem` in `pipeline/predict.py` branches on the *name* containing `"anytime_td"` to decide whether to use `predict_proba` or `predict`.
  - Some other problems are classification or quasi‑probabilistic (e.g. availability, possibly some variants later), but they’re treated as regression.
  - Effect:
    - The guard code (`_apply_guards_inline`, `_apply_availability_floor`) was clearly written assuming probabilities, not hard labels; with 0/1 inputs many downstream thresholds and scalings behave poorly.
    - The TD model chain ends up consuming mismatched semantics relative to training.

---

**5. Future data leakage / label-derived leakage**

Most of the heavy leakage concerns have been actively guarded against, and the as‑of machinery is solid:

- **Same‑game box score leakage is centrally blocked**
  - `LEAK_PRONE_COLUMNS` in `utils/general/constants.py` lists all same‑game outcomes: yards, attempts, TDs, snaps, etc.
  - `config/training.yaml` mirrors this via `_shared_columns_to_discard` and `_columns_to_discard_extended` and tests enforce consistency (`tests/test_training_config_features.py`).
  - Training feature selection logic uses those discard lists so same‑game yards/targets/snaps/TDs are not used as features for usage/efficiency/anytime models.

- **As-of cutoffs for injuries/odds/weather are respected**
  - `utils/feature/asof.py` + `asof_metadata` define `decision_cutoff_ts` per game/horizon.
  - In `pipeline/feature.py`:
    - You join `asof_meta` onto player-game rows and drop any rows where snapshot timestamps exceed `decision_cutoff_ts` (injury, roster, odds, forecast) and write violations to audit files (`pipeline/feature.py:376–488`).
    - `drop_missing_snapshots_enabled()` optionally drops rows without any pre‑cutoff snapshots, preventing silent leakage from “future default” data.
  - Injury timelines inside `player_game_level` recompute a `decision_cutoff_ts` via `get_decision_cutoff_hours()` and filter to `reported_at <= decision_cutoff_ts` (`utils/feature/player_game_level.py:1760+`), inside the same `decision_cutoff_override` context used by `build_feature_matrix`.
  - Odds and weather feature builders consult `decision_cutoff_ts` and game start time to choose snapshot times.

- **Derived target usage is mostly safe**
  - For efficiency yards models, `derived_target` with `external_source: 'player_game'` pulls `receiving_yards` / `rushing_yards` from `PLAYER_GAME_DIR` (`pipeline/train.py:760–820`).
  - Those stats are only used as **labels**, while the same columns are dropped as features via `columns_to_discard`.
  - Purged group time series split with `group_col: 'game_id'` and embargo ensures label info doesn’t leak across train/val splits.

Where there is conceptual leakage risk:

- **Pre‑snap features act as de‑facto label proxies for that game**
  - Because `ps_*` stats are computed from full play-by-play of that same game, they encode the realized script and target tree.
  - They’re used both as labels (`ps_route_participation_pct`, `ps_scripted_touches`) and as features (their own counts, shares, positional splits) without any “pre‑cutoff” constraint.
  - In real life you don’t know which WRs will be heavily scripted in **this** game; you only know historical tendencies and hints from injuries/odds.
  - So even if they’re not literally future “snapshots,” they are **post‑snap outcomes** being fed into upstream layers of the chain.

---

**6. Dead / noisy data sources and features**

- **Pre‑snap baselines for seasons without tracking**
  - `_load_ps_baselines` in `pipeline/predict.py:520–560` tries to backfill `PS_BASELINE_COLUMNS` from previous seasons’ `PLAYER_GAME_DIR` partitions.
  - If tracking is missing or schema changed, it silently returns an empty frame and `_apply_ps_fallback` becomes a no‑op.
  - Where tracking coverage is partial (early seasons, some teams), `ps_*` inputs to models are a mix of:
    - Real pre‑snap tracking for some players and games.
    - Forward‑filled baselines for others.
    - Zeros for the rest.
  - Effect:
    - For these cohorts, `ps_*` features are effectively noise; the model will overfit to segments with tracking and underperform on segments without, unless you stratify or drop them.

- **Legacy LightGBM configuration, MLB‑selective modules, etc.**
  - Various modules are clearly marked legacy MLB (`utils/train/selective/*`, docs `mlb_legacy.md`, etc.) and not wired into the NFL path.
  - These are harmless but clutter the mental model; for NFL‑only work they’re dead weight.

- **BASE_GAME_COLS in prediction mostly zero for future games**
  - `BASE_GAME_COLS` in `pipeline/predict.py:145–180` includes outcomes like `passing_yards`, `rushing_yards`, `touchdowns`, `td_count`, etc.
  - For upcoming games these are inherently unknown and end up as zeros or missing; inference artifacts should be excluding them anyway via the saved feature list.
  - Effect:
    - If any of these leak into the active feature set for inference due to a mismatch in artifacts, they’re just constant zeros—noise at best.

---

**7. Hallucinated / legacy cruft**

- **Duplicated `efficiency_tds` block looks like an editing artifact**
  - As noted above, the double definition in `config/training.yaml` is extremely likely to be a copy‑paste or model‑editing artifact, not intentional design.
  - It should be cleaned to a single, well‑documented TD efficiency problem.

- **Pre‑snap model stack appears partially designed by “wish list” rather than data reality**
  - The pre‑snap models’ feature sets (`ps_route_participation_`, `ps_targets_`, `team_red_zone_`, `player_red_zone_`, etc.) clearly encode a conceptual, multi‑layer architecture:
    - First predict routes and scripted touches.
    - Feed those into usage.
    - Then efficiency.
  - But the actual implementation:
    - Doesn’t ensure that the **same kind of inputs** are available at inference (history vs current game mismatch).
    - Uses quite a few prefixes (`ps_total_`, `ps_targets_slot_share_*`, etc.) that may be sparsely populated in older seasons or for low‑usage players.
  - This is more “aspirational architecture” than tightly validated reality and is a good candidate for pruning/grounding.

---

**Concrete recommendations (short list)**

- **Align train/inference for `ps_*` features**:
  - Either:
    - Re‑define `ps_*` features to be rolling history only (and rename current‑game versions), or
    - Train pre‑snap models using *only* the same rolling historical `ps_*` features that inference uses.
- **Fix prediction of intermediate classification problems**:
  - Treat all classification problems (at minimum `availability_active`, `anytime_td`) as requiring `predict_proba`, not `predict`, and ensure the predictor loads calibrated wrappers if training saved them.
- **Deduplicate and simplify config**:
  - Collapse the duplicated `efficiency_tds` problem in `config/training.yaml` into a single canonical definition.
  - Remove or clearly comment unused sections (LightGBM config, unused problems).
- **Audit pre‑snap model feature lists**:
  - Remove any same‑game ps_* features from downstream problems that are not realistically knowable pre‑kickoff; rely on historical/role and odds/injury context instead.
- **Treat low‑coverage pre‑snap tracking as optional**:
  - For seasons/players without reliable tracking, drop or down‑weight ps_* features rather than silently filling them—otherwise they behave like structured noise.

If you’d like, next step could be to prioritize these into an actionable change list (e.g., “fix classification predict_proba” and “align ps_* semantics”) and then refactor specific pieces of the pipeline.
    <END_ANALYSIS_END>

    This step is PURELY research, investigation, and planning.
    Do NOT make any edits to the code or data in this step.

    I want:
    - A structured, prioritized plan.
    - Clear steps that can be implemented by an agent in later steps.
    - Notes on risk or potential pitfalls where relevant, but focus on high-value changes.
    - Explicit attention to:
        * eliminating data leakage,
        * aligning features with real football mechanisms that drive anytime TDs,
        * cleaning up hallucinated / legacy cruft that no longer makes sense.

    Again: no edits here, just the plan.

mcp startup: no servers





thinking
**Creating a project plan**

The user wants a rich, detailed plan without any code edits. They have asked for a structured, prioritized plan for future implementation, so I need to focus on that. It seems using the update_plan tool isn't necessary since it's more about research, not internal steps. I won't call any tools and will instead rely on the analysis already provided by the user to produce a thorough plan document. I'll ensure it's well organized and informative!
thinking
**Drafting a structured project plan**

I need to create a detailed project plan that includes a clear structure with sections, prioritization into phases or tiers, and actionable steps for future execution. I'll focus on risks like eliminating data leakage and aligning features with real football metrics. The plan could be organized into epics, such as "Epic 1: ps_* alignment," followed by phases: analysis, design, implementation, and validation. I’ll include specific checklists and a top-level roadmap detailing phases, epics, and steps to ensure it's comprehensive and actionable!
thinking
**Addressing potential pitfalls and tasks**

I want to discuss potential pitfalls, such as data breaks and distribution shifts that could occur when removing strong but leaky features. It's crucial to conduct backtests to validate our changes. While I can’t use citations in a specific format, I can refer to lines in the code with references like `pipeline/predict.py:2286`. I'll outline tasks like modifying `_predict_for_problem` to use model metadata for classification versus regression. I’ll prioritize tasks into tiers and explicitly mention eliminating data leakage, ensuring that features are based on as-of data and designed appropriately.
thinking
**Restructuring training features**

I think it’s wise to introduce distinct namespaces, like `ps_hist_*` for historical data and `ps_game_*` for current game data. The models should use the `ps_hist_*` data to approximate pre-snap plans while keeping `ps_game_*` as optional labels for prediction. I'll also focus on adding tests to ensure no same-game columns slip in and treat `availability_active` as a probability. I’ll clean up the configuration by removing duplicates, documenting, and aiming for high-value changes like fixing classification and alignment first, all while respecting the user's request for no edits at this step.
thinking
**Creating a detailed project plan**

The user is asking for a rich and detailed project plan, which means I can be expansive despite the system's baseline constraints. I'll structure the plan with an overview and priorities first, then dive into detailed sections by epic. Each epic will include subsections for objectives, steps, risks, and checks. I’ll also integrate the user’s specific focus points like data leakage, football realism, and removing unnecessary elements. My goal is to ensure it’s both concise and thorough, aiming for around 1500-2500 words. Let’s get started!
codex
Here’s a roadmap that a future agent (or you) could execute step‑by‑step. I’ll prioritize by impact, then drill into detailed steps, risks, and checks.

---

**Top‑Level Priorities**

- **Tier 1 (High impact, relatively contained):**
  - T1.1: Align `ps_*` train vs inference semantics and remove same‑game leakage.
  - T1.2: Fix classification pipelines to use probabilities end‑to‑end (availability, anytime TD).
  - T1.3: Deduplicate and simplify TD efficiency / anytime TD configs.

- **Tier 2 (Important but slightly more invasive):**
  - T2.1: Re‑design pre‑snap feature namespace (historical vs same‑game) and audit them out of downstream problems where unrealistic.
  - T2.2: Make pre‑snap tracking coverage explicit; treat missing data sensibly.
  - T2.3: Refine scripted touches / early‑game heuristics to better match real offensive design.

- **Tier 3 (Cleanup, observability, polish):**
  - T3.1: Prune legacy/unused model configs and modules.
  - T3.2: Add invariant tests and monitoring around feature parity and leakage.
  - T3.3: Document football‑mechanism assumptions and how they map into features.

Below, I’ll describe each “epic” with concrete implementation steps and pitfalls.

---

**Epic T1.1 – Align `ps_*` semantics & eliminate same‑game leakage**

**Goal:** Ensure any pre‑snap features used as inputs at training are built from the same type of information available at prediction time (historical tracking, injuries, odds), not realized same‑game outcomes. Same‑game `ps_*` stats become labels or diagnostic outputs, not features for upstream models.

**Design decision (choose one path, but plan supports both):**

- **Option A (Recommended):**  
  - `ps_game_*` = realized same‑game pre‑snap stats, used only as labels.
  - `ps_hist_*` = rolling historical estimates (what inference currently uses, e.g. `1g_ps_route_participation_pct_per_game`, `3g_*`).
  - Models that approximate “pre‑snap plan” train solely on `ps_hist_*` + contextual features, predicting `ps_game_*` as labels.
- **Option B:**  
  - Retain `ps_*` as historical by definition (rename current same‑game fields to something else and keep them out of features).

I’ll assume Option A for concreteness.

**Steps:**

1. **Audit where `ps_*` is computed and used**
   - Identify:
     - Where same‑game `ps_*` is created in `utils/feature/player_game_level.py` (e.g. `_compute_pre_snap_usage`, route participation, scripted touches).
     - Where rolling `ps_*` per‑game features are created in training (feature pipeline) and inference (`pipeline/predict.py → add_rolling_features` and renaming).
     - All problems referencing `ps_*` columns in `config/training.yaml` (`feature_prefixes_to_include`, `other_features_to_include`).
   - Deliverable: a short mapping doc listing:
     - `ps_game_*` columns (same‑game).
     - `ps_hist_*` columns (rolling historical).
     - Which problems currently use which.

2. **Introduce clear namespaces for pre‑snap metrics**
   - In the feature builders:
     - Rename same‑game pre‑snap columns to `ps_game_*` (e.g., `ps_route_participation_pct` → `ps_game_route_participation_pct`).
     - Ensure they are **never** used as features; mark them as labels or “diagnostic targets” in training config.
   - For historical/pre‑snap projections:
     - Ensure rolling per‑game features carry a separate prefix, e.g. `ps_hist_*` or `ps_roll_*`.  
       - Example: `1g_ps_route_participation_pct_per_game` → `ps_hist_route_participation_pct_1g`.
       - `3g_ps_route_participation_pct_per_game` → `ps_hist_route_participation_pct_3g`.
     - Update the renaming logic in prediction to **not** alias them to names that collide with same‑game stats.

3. **Refine model configs to use only historical pre‑snap inputs**
   - For `pre_snap_routes`, `pre_snap_scripted_touches`, and any upstream pre‑snap problems:
     - Change `feature_prefixes_to_include` to include `ps_hist_*` (and contextual stuff like `injury_`, `team_`, `odds_`, `defense_`).
     - Explicitly exclude `ps_game_*` and all other same‑game realized stats via `columns_to_discard`.
   - For downstream problems (usage, efficiency, anytime TDs):
     - Ensure no `ps_game_*` columns are present as features:
       - If they are, either:
         - Replace with `ps_hist_*` analogs, or
         - Drop them if there is no sensible historical proxy.

4. **Re‑wire the pre‑snap model stack**
   - Make the pipeline explicit:
     - Stage 1: Historical & context (`ps_hist_*`, injuries, odds, team tendencies) → predict `ps_game_*` labels (`routes`, `scripted_touches`, alignment).
     - Stage 2: Use predicted `ps_game_*` or their expectations as inputs into usage/efficiency models.
   - Concretely:
     - Modify `_inject_composed_features` in `pipeline/predict.py` to:
       - Take the predicted pre‑snap distributions (e.g. expected routes, expected scripted touches) and feed them into target/carry distribution models.
       - Avoid directly using realized same‑game stats (they won’t exist at inference anyway).

5. **Align training & prediction feature selection artifacts**
   - Regenerate and persist feature lists per problem after the namespace change.
   - Update prediction code to always:
     - Load the exact feature list saved during training.
     - Enforce that `ps_game_*` columns are **never required** as inputs to inference models (they may exist in data for diagnostics, but not in `X`).

6. **Add invariants / tests for leakage and parity**
   - Tests (or assertions) to add:
     - For each problem, assert that any column with suffix `_yards`, `_targets`, `_tds`, `ps_game_*` is never in the “input features” list.
     - For the set of features used in training vs prediction for a given problem:
       - They match exactly, or differences are explicitly enumerated (e.g., extra diagnostic columns that are ignored at inference).
     - A dedicated “no same‑game `ps_game_*` as feature” check in `tests/test_training_config_features.py` or equivalent.

**Risks / Pitfalls:**

- Removing strong same‑game `ps_*` inputs can **reduce short‑term model performance metrics** (AUC/R²) because you’re taking away label‑adjacent inputs. That’s expected and healthy.
- Renaming `ps_*` columns requires careful migration of:
  - Saved models.
  - Downstream reporting code and notebooks.
- Need to ensure no silent back‑compat fallbacks (e.g., try/except dropping missing columns) mask broken pipelines.

---

**Epic T1.2 – Fix classification pipeline & probability semantics**

**Goal:** Ensure all classification models (availability, anytime TD, and any others) are trained, stored, and used as probability estimators, not hard thresholds.

**Steps:**

1. **Inventory all classification‑type problems**
   - From `config/training.yaml`, identify problems with:
     - `problem_type: classification` or similar.
     - Binary labels (e.g., `availability_active`, `anytime_td`, maybe some injury/roster problems).
   - Document expected outputs (probabilities vs labels) for each.

2. **Standardize model saving/loading for classifiers**
   - Confirm how calibration is applied in training:
     - If you wrap base models with calibrators (e.g., Platt scaling, isotonic), ensure the wrapper is what gets saved.
   - Define a clear interface for loaded models:
     - All classification models must support `predict_proba(X)[:, 1]`.
   - If some models are stored without calibrator:
     - Either migrate to saving calibrated wrappers.
     - Or store metadata about calibration in the artifact and re‑apply on load.

3. **Refactor `_predict_for_problem` to use model metadata, not name heuristics**
   - Instead of `if "anytime_td" in problem_name: use predict_proba else predict`, do:
     - Load problem metadata (e.g. from config or a small manifest saved with the model) specifying:
       - `problem_type` (`classification`, `regression`).
       - `output_mode` (`probability`, `count`, etc.).
     - If `problem_type == "classification"`, always call `predict_proba`.
   - Ensure special cases (like multi‑class, if any) are handled explicitly.

4. **Update downstream usage of classification outputs**
   - For `availability_active`:
     - Treat `p_active = model.predict_proba(X)[:, 1]` as a probability.
     - Use `p_active` directly in expected value calculations (e.g., `expected_targets *= p_active`).
     - If you want a binary flag for some guardrails, derive it via thresholding (e.g., `p_active > 0.5`), but **keep both**.
   - For `anytime_td`:
     - Ensure chain uses `p_td` as a probability:
       - When combining with usage/efficiency models, keep the semantics clear: `expected_td = p_anytime_td` vs `expected_td_count` from Poisson approximations, etc.

5. **Guard against accidental integer clipping / casting**
   - Review any code where outputs are:
     - Cast to `int` or `round`ed prematurely.
     - Clipped to `[0, 1]` under the assumption they’re probabilities, but coming from `predict()` of regression models.
   - Make sure clipping is applied **only** to values that are logically probabilities.

6. **Validation & regression checks**
   - Recompute:
     - Calibration curves / Brier scores for `availability_active` and `anytime_td`.
     - Compare pre‑change vs post‑change (you should see better calibration, maybe slightly different AUC).
   - Add tests that:
     - For classification problems, `predict_proba` is used and output values are between 0 and 1.
     - Downstream functions that expect probabilities receive `float` arrays, not `int` labels.

**Risks / Pitfalls:**

- Models trained with assumptions about `predict()` vs `predict_proba()` might require minor retraining or calibration.
- If some classification problems are currently implemented as regression (e.g., target `0/1` with regression objective), you’ll need to either:
  - Convert them to true classification objectives, or
  - Treat their outputs as scores and calibrate separately.

---

**Epic T1.3 – Deduplicate and simplify TD efficiency / anytime TD configs**

**Goal:** Have a single, clear, auditable configuration for TD‑related models with no duplicate `efficiency_tds` definitions or conflicting feature sets.

**Steps:**

1. **Clarify the intended TD efficiency problem definition**
   - Decide with domain logic:
     - What is the canonical label (`tds_per_opportunity`, `prob_td_on_touch`, etc.)?
     - Which feature families should TD efficiency use (usage, red zone roles, team totals, pace, defense, etc.)?
   - Capture this in a short design note.

2. **Resolve the duplicate `efficiency_tds` blocks**
   - Compare the two blocks in `config/training.yaml`:
     - Feature prefixes.
     - Columns to discard.
     - Any differences in hyperparams.
   - Choose one as canonical (or merge them):
     - E.g., keep the richer feature set but drop obviously redundant or leaky prefixes (`ps_game_*`, same‑game outcomes).
   - Remove the duplicate block and update any references that rely on it.

3. **Align anytime TD model configuration with the efficiency chain**
   - Document the chain:
     - Availability → usage (targets/carries) → efficiency (yards/TD per opportunity) → anytime TD.
   - Ensure:
     - Anytime TD config uses:
       - Probabilities and expected counts from upstream problems.
       - Contextual features like team totals, game totals, pace.
     - Avoid re‑using raw same‑game stats or label proxies as features.

4. **Simplify modeling options**
   - If LightGBM isn’t used:
     - Either remove it from `models:` or clearly comment it as “experimental/off by default”.
   - Ensure `training.models_to_train` includes only the models you actually train and deploy (e.g., `['xgboost']`).

5. **Add consistency checks**
   - Test to assert no duplicate `name:` entries in `problems` config.
   - Add a small script or test that:
     - Loads all problem definitions.
     - Checks for unique names and consistent `problem_type`.

**Risks / Pitfalls:**

- Removing a duplicate block changes behavior if production unexpectedly relies on the “second” version; you’ll want to re‑run a backtest to confirm performance stays acceptable or improves.

---

**Epic T2.1 – Pre‑snap feature audit and realistic football semantics**

**Goal:** Ensure every pre‑snap feature reflects something knowable pre‑kickoff and ties clearly to real mechanisms: role, alignment, script, red zone usage, team tendencies.

**Steps:**

1. **Categorize pre‑snap features by realism**
   - For each `ps_*` (or new `ps_hist_*`/`ps_game_*`) prefix, classify:
     - **Realistic pre‑game info:** historical role, alignment tendencies, red zone share, scripted usage trends.
     - **Borderline:** things that correlate with game plan but may be noisy (e.g., early‑game pace within the current season).
     - **Unrealistic (leaky):** same‑game realized stats, direct label proxies (e.g., same‑game TD shares).
   - Document per feature family: `ps_route_participation_*`, `ps_targets_*`, `ps_scripted_*`, red zone splits, etc.

2. **Prune or rename unrealistic pre‑snap features**
   - For features that are “unrealistic” but currently used:
     - Either:
       - Remove them as inputs (preferred).
       - Or redesign them as historical aggregates (e.g., last 8 games, season to date).
   - Ensure new names reflect semantics:
     - `rz_target_share_last_8g` rather than `player_red_zone_target_share`.

3. **Re‑balance feature sets around real drivers**
   - Emphasize:
     - Team implied totals and spreads (odds).
     - Historical red zone role (`targets inside 10`, `carries inside 5`, etc. per player).
     - Positional and formation tendencies (slot vs wide vs backfield usage).
     - Pace, pass rate over expected, run/pass splits by game script.
   - De‑emphasize:
     - Features that mix realized “hot hand” from the current game with historical role.

4. **Define small, interpretable pre‑snap feature groups**
   - Create 3–5 conceptual groups:
     - `role_usage` (routes, snaps, alignment history).
     - `scripted_intent` (openers, 3rd‑down usage).
     - `scoring_context` (team totals, red zone history).
     - `game_environment` (pace, spread, weather).
   - Ensure each pre‑snap or TD model uses these groups in ways that match football logic.

5. **Add documentation**
   - Write brief docs (even in comments or a markdown file) describing:
     - What each group means.
     - How it ties to real decision‑making (e.g., “this approximates OC’s likely red zone plan”).

**Risks / Pitfalls:**

- Over‑simplifying may hurt raw accuracy; need to balance football realism with empirical performance.
- Some features may have overlapping information; regularization/hyperparameters may need retuning after pruning.

---

**Epic T2.2 – Handling low‑coverage pre‑snap tracking and baselines**

**Goal:** Make tracking coverage and baseline fallback explicit, avoiding silent “noisy filler” behavior for players/seasons without tracking.

**Steps:**

1. **Make tracking coverage visible**
   - Add indicators:
     - `has_tracking_this_season`, `has_tracking_last_3g`, etc.
   - When `_load_ps_baselines` succeeds/fails, record:
     - A flag column: `ps_baseline_source` (`actual`, `baseline`, `none`).

2. **Define a clear fallback strategy**
   - Policies for missing tracking:
     - If no tracking and no meaningful baseline:
       - Either drop those player‑game rows from pre‑snap models, or
       - Use heavily shrunk priors (e.g., depth‑chart based role) with explicit indicator flags.
   - For low‑usage players:
     - Consider hierarchical pooling (e.g., use positional averages by team/role).

3. **Update `_apply_ps_fallback` to be explicit, not silent**
   - Instead of a no‑op on empty baseline:
     - Set `ps_hist_*` to `NaN` or a defined prior and mark the row via an indicator.
   - Ensure models:
     - Either handle missing values robustly, or
     - Use the indicator to “know” that this is a low‑info case and rely more on team/position context.

4. **Stratify evaluation by tracking coverage**
   - When evaluating pre‑snap and TD models:
     - Slice metrics by `has_tracking_*` and `ps_baseline_source`.
     - Confirm that:
       - Performance on fully tracked cohorts is good.
       - Performance on low‑coverage cohorts is at least reasonable, not driven by noise.

**Risks / Pitfalls:**

- Dropping low‑coverage rows reduces sample size; may need regularization tweaks.
- Introducing NaNs requires careful handling in feature matrices and model training.

---

**Epic T2.3 – Refine scripted touches / early‑game heuristics**

**Goal:** Move from a rigid “first 15 plays” definition to something that captures how teams actually script drives and touches.

**Steps:**

1. **Analyze current scripted heuristic**
   - Use existing `offense_play_rank < 15` scripted labels to:
     - Compute per‑team distribution of scripted plays per game.
     - See whether 15 is a good cutoff across teams/seasons.
   - Look for:
     - Two‑minute drills, hurry‑up sequences that are mis‑classified.
     - Games where the first 15 plays span multiple drives with very different context.

2. **Design a more nuanced scripted definition**
   - Candidate heuristics:
     - “Scripted” = plays in first X minutes of game clock on early downs outside two‑minute/4‑minute situations.
     - Or: first N plays **per offense** excluding hurry‑up scenarios (no huddles, < 1:00 on clock).
   - Optionally:
     - Design team‑level `script_length` parameters learned from historical behavior.

3. **Adjust labels and recompute `ps_game_scripted_*`**
   - Implement improved heuristics in `player_game_level`:
     - New flag `is_scripted_play`.
     - Recompute `ps_game_scripted_targets`, `ps_game_scripted_carries`, etc.
   - Use historical data to confirm:
     - New scripted labels correlate with expected openers (WR1/WR2, goal‑line RBs, etc.).

4. **Re‑train pre‑snap models that use scripted features**
   - Update pre‑snap models that include scripted touch features:
     - Use `ps_hist_*` approximations to predict new `ps_game_scripted_*` labels.
   - Re‑evaluate:
     - How much scripted touches add over generic role metrics to anytime TD prediction.

**Risks / Pitfalls:**

- More complex heuristics may be harder to reason about; keep them as simple as possible while fixing major issues.
- Some teams may have inconsistent scripting; avoid overfitting to quirks in small samples.

---

**Epic T3.1 – Cleanup of legacy / unused code**

**Goal:** Reduce cognitive load and risk of misinterpretation by clearly marking or removing unused configs and modules.

**Steps:**

1. **Identify unused model families and configs**
   - LightGBM blocks in `config/training.yaml`.
   - Legacy MLB code under `utils/train/selective/*`, `mlb_legacy.md`, etc.
   - Any old problems in `problems` list that are not:
     - Trained.
     - Saved.
     - Used downstream in prediction.

2. **Decide policy: remove vs quarantine**
   - For NFL work, likely:
     - Move legacy configs into a `legacy/` or `archive/` section with explicit comments.
     - Or delete entirely if not referenced.

3. **Update documentation**
   - Document the active set of problems and model types:
     - E.g., “Active for NFL anytime TD pipeline: availability_active, pre_snap_routes, pre_snap_scripted_touches, usage_targets, usage_carries, efficiency_yards, efficiency_tds, anytime_td.”

4. **Add CI/Tests for config hygiene**
   - Test that:
     - All `problems.name` values are unique.
     - All referenced problems have corresponding model artifacts.
     - No model type is declared in `config` but never used anywhere.

**Risks / Pitfalls:**

- Removing code that is used in ad‑hoc scripts or by other teams. Favor quarantining with documentation over hard deletion unless you’re sure.

---

**Epic T3.2 – Observability & leakage guards**

**Goal:** Add automated checks that protect against reintroducing leakage or train/predict feature mismatches.

**Steps:**

1. **Feature parity tests**
   - For each problem:
     - Load training feature names from artifacts.
     - Load prediction feature names from the live pipeline.
     - Assert parity, with a small list of allowed differences (e.g., `game_id`, `player_id`).

2. **Leakage signature tests**
   - Maintain lists of “never features”:
     - Same‑game outcomes: `*_yards`, `*_td`, `*_touches`, `*_targets`, etc.
     - `ps_game_*` namespace.
   - Test that:
     - These columns never appear in `features_to_use` for any problem.

3. **As‑of and snapshot tests**
   - Add regression tests ensuring:
     - `decision_cutoff_ts` logic in `feature.py` and `player_game_level.py` correctly filters injury/odds/weather columns.
     - No columns with `reported_at` > `decision_cutoff_ts` survive into training matrices.

4. **Monitoring / dashboards (if applicable)**
   - For production predictions:
     - Track distributions of key features and outputs over time.
     - Alert on distribution shifts, especially for:
       - `ps_hist_*` inputs.
       - `p_active`, `p_anytime_td`.

**Risks / Pitfalls:**

- Over‑constraining tests can slow iteration; keep rules minimal but high‑impact.

---

**Epic T3.3 – Documentation of football mechanisms**

**Goal:** Make the mapping from football concepts to features and models explicit so future changes stay aligned with reality.

**Steps:**

1. **Write a short “anytime TD model design” doc**
   - Sections:
     - High‑level architecture (availability → usage → efficiency → TD).
     - Definitions of key labels (what is “anytime TD” exactly, what is “efficiency_tds”).
     - Explanation of pre‑snap models and how their predictions are used.

2. **Document feature groups and semantics**
   - For each group (role_usage, scripted_intent, scoring_context, game_environment):
     - List representative features.
     - Explain their intended connection to TD probability.

3. **Add inline references from config to docs**
   - In `config/training.yaml`:
     - Add short comments or references to the design doc sections next to each problem.
   - Optionally, generate a markdown summary from the config so docs stay in sync.

---

If you’d like, next step could be to pick one Tier‑1 epic (e.g., `ps_*` alignment or classification pipeline) and I can turn that into a more concrete implementation checklist (file‑by‑file changes, data migration steps, and specific tests to add), ready for an agent to execute.
tokens used
10,050
Here’s a roadmap that a future agent (or you) could execute step‑by‑step. I’ll prioritize by impact, then drill into detailed steps, risks, and checks.

---

**Top‑Level Priorities**

- **Tier 1 (High impact, relatively contained):**
  - T1.1: Align `ps_*` train vs inference semantics and remove same‑game leakage.
  - T1.2: Fix classification pipelines to use probabilities end‑to‑end (availability, anytime TD).
  - T1.3: Deduplicate and simplify TD efficiency / anytime TD configs.

- **Tier 2 (Important but slightly more invasive):**
  - T2.1: Re‑design pre‑snap feature namespace (historical vs same‑game) and audit them out of downstream problems where unrealistic.
  - T2.2: Make pre‑snap tracking coverage explicit; treat missing data sensibly.
  - T2.3: Refine scripted touches / early‑game heuristics to better match real offensive design.

- **Tier 3 (Cleanup, observability, polish):**
  - T3.1: Prune legacy/unused model configs and modules.
  - T3.2: Add invariant tests and monitoring around feature parity and leakage.
  - T3.3: Document football‑mechanism assumptions and how they map into features.

Below, I’ll describe each “epic” with concrete implementation steps and pitfalls.

---

**Epic T1.1 – Align `ps_*` semantics & eliminate same‑game leakage**

**Goal:** Ensure any pre‑snap features used as inputs at training are built from the same type of information available at prediction time (historical tracking, injuries, odds), not realized same‑game outcomes. Same‑game `ps_*` stats become labels or diagnostic outputs, not features for upstream models.

**Design decision (choose one path, but plan supports both):**

- **Option A (Recommended):**  
  - `ps_game_*` = realized same‑game pre‑snap stats, used only as labels.
  - `ps_hist_*` = rolling historical estimates (what inference currently uses, e.g. `1g_ps_route_participation_pct_per_game`, `3g_*`).
  - Models that approximate “pre‑snap plan” train solely on `ps_hist_*` + contextual features, predicting `ps_game_*` as labels.
- **Option B:**  
  - Retain `ps_*` as historical by definition (rename current same‑game fields to something else and keep them out of features).

I’ll assume Option A for concreteness.

**Steps:**

1. **Audit where `ps_*` is computed and used**
   - Identify:
     - Where same‑game `ps_*` is created in `utils/feature/player_game_level.py` (e.g. `_compute_pre_snap_usage`, route participation, scripted touches).
     - Where rolling `ps_*` per‑game features are created in training (feature pipeline) and inference (`pipeline/predict.py → add_rolling_features` and renaming).
     - All problems referencing `ps_*` columns in `config/training.yaml` (`feature_prefixes_to_include`, `other_features_to_include`).
   - Deliverable: a short mapping doc listing:
     - `ps_game_*` columns (same‑game).
     - `ps_hist_*` columns (rolling historical).
     - Which problems currently use which.

2. **Introduce clear namespaces for pre‑snap metrics**
   - In the feature builders:
     - Rename same‑game pre‑snap columns to `ps_game_*` (e.g., `ps_route_participation_pct` → `ps_game_route_participation_pct`).
     - Ensure they are **never** used as features; mark them as labels or “diagnostic targets” in training config.
   - For historical/pre‑snap projections:
     - Ensure rolling per‑game features carry a separate prefix, e.g. `ps_hist_*` or `ps_roll_*`.  
       - Example: `1g_ps_route_participation_pct_per_game` → `ps_hist_route_participation_pct_1g`.
       - `3g_ps_route_participation_pct_per_game` → `ps_hist_route_participation_pct_3g`.
     - Update the renaming logic in prediction to **not** alias them to names that collide with same‑game stats.

3. **Refine model configs to use only historical pre‑snap inputs**
   - For `pre_snap_routes`, `pre_snap_scripted_touches`, and any upstream pre‑snap problems:
     - Change `feature_prefixes_to_include` to include `ps_hist_*` (and contextual stuff like `injury_`, `team_`, `odds_`, `defense_`).
     - Explicitly exclude `ps_game_*` and all other same‑game realized stats via `columns_to_discard`.
   - For downstream problems (usage, efficiency, anytime TDs):
     - Ensure no `ps_game_*` columns are present as features:
       - If they are, either:
         - Replace with `ps_hist_*` analogs, or
         - Drop them if there is no sensible historical proxy.

4. **Re‑wire the pre‑snap model stack**
   - Make the pipeline explicit:
     - Stage 1: Historical & context (`ps_hist_*`, injuries, odds, team tendencies) → predict `ps_game_*` labels (`routes`, `scripted_touches`, alignment).
     - Stage 2: Use predicted `ps_game_*` or their expectations as inputs into usage/efficiency models.
   - Concretely:
     - Modify `_inject_composed_features` in `pipeline/predict.py` to:
       - Take the predicted pre‑snap distributions (e.g. expected routes, expected scripted touches) and feed them into target/carry distribution models.
       - Avoid directly using realized same‑game stats (they won’t exist at inference anyway).

5. **Align training & prediction feature selection artifacts**
   - Regenerate and persist feature lists per problem after the namespace change.
   - Update prediction code to always:
     - Load the exact feature list saved during training.
     - Enforce that `ps_game_*` columns are **never required** as inputs to inference models (they may exist in data for diagnostics, but not in `X`).

6. **Add invariants / tests for leakage and parity**
   - Tests (or assertions) to add:
     - For each problem, assert that any column with suffix `_yards`, `_targets`, `_tds`, `ps_game_*` is never in the “input features” list.
     - For the set of features used in training vs prediction for a given problem:
       - They match exactly, or differences are explicitly enumerated (e.g., extra diagnostic columns that are ignored at inference).
     - A dedicated “no same‑game `ps_game_*` as feature” check in `tests/test_training_config_features.py` or equivalent.

**Risks / Pitfalls:**

- Removing strong same‑game `ps_*` inputs can **reduce short‑term model performance metrics** (AUC/R²) because you’re taking away label‑adjacent inputs. That’s expected and healthy.
- Renaming `ps_*` columns requires careful migration of:
  - Saved models.
  - Downstream reporting code and notebooks.
- Need to ensure no silent back‑compat fallbacks (e.g., try/except dropping missing columns) mask broken pipelines.

---

**Epic T1.2 – Fix classification pipeline & probability semantics**

**Goal:** Ensure all classification models (availability, anytime TD, and any others) are trained, stored, and used as probability estimators, not hard thresholds.

**Steps:**

1. **Inventory all classification‑type problems**
   - From `config/training.yaml`, identify problems with:
     - `problem_type: classification` or similar.
     - Binary labels (e.g., `availability_active`, `anytime_td`, maybe some injury/roster problems).
   - Document expected outputs (probabilities vs labels) for each.

2. **Standardize model saving/loading for classifiers**
   - Confirm how calibration is applied in training:
     - If you wrap base models with calibrators (e.g., Platt scaling, isotonic), ensure the wrapper is what gets saved.
   - Define a clear interface for loaded models:
     - All classification models must support `predict_proba(X)[:, 1]`.
   - If some models are stored without calibrator:
     - Either migrate to saving calibrated wrappers.
     - Or store metadata about calibration in the artifact and re‑apply on load.

3. **Refactor `_predict_for_problem` to use model metadata, not name heuristics**
   - Instead of `if "anytime_td" in problem_name: use predict_proba else predict`, do:
     - Load problem metadata (e.g. from config or a small manifest saved with the model) specifying:
       - `problem_type` (`classification`, `regression`).
       - `output_mode` (`probability`, `count`, etc.).
     - If `problem_type == "classification"`, always call `predict_proba`.
   - Ensure special cases (like multi‑class, if any) are handled explicitly.

4. **Update downstream usage of classification outputs**
   - For `availability_active`:
     - Treat `p_active = model.predict_proba(X)[:, 1]` as a probability.
     - Use `p_active` directly in expected value calculations (e.g., `expected_targets *= p_active`).
     - If you want a binary flag for some guardrails, derive it via thresholding (e.g., `p_active > 0.5`), but **keep both**.
   - For `anytime_td`:
     - Ensure chain uses `p_td` as a probability:
       - When combining with usage/efficiency models, keep the semantics clear: `expected_td = p_anytime_td` vs `expected_td_count` from Poisson approximations, etc.

5. **Guard against accidental integer clipping / casting**
   - Review any code where outputs are:
     - Cast to `int` or `round`ed prematurely.
     - Clipped to `[0, 1]` under the assumption they’re probabilities, but coming from `predict()` of regression models.
   - Make sure clipping is applied **only** to values that are logically probabilities.

6. **Validation & regression checks**
   - Recompute:
     - Calibration curves / Brier scores for `availability_active` and `anytime_td`.
     - Compare pre‑change vs post‑change (you should see better calibration, maybe slightly different AUC).
   - Add tests that:
     - For classification problems, `predict_proba` is used and output values are between 0 and 1.
     - Downstream functions that expect probabilities receive `float` arrays, not `int` labels.

**Risks / Pitfalls:**

- Models trained with assumptions about `predict()` vs `predict_proba()` might require minor retraining or calibration.
- If some classification problems are currently implemented as regression (e.g., target `0/1` with regression objective), you’ll need to either:
  - Convert them to true classification objectives, or
  - Treat their outputs as scores and calibrate separately.

---

**Epic T1.3 – Deduplicate and simplify TD efficiency / anytime TD configs**

**Goal:** Have a single, clear, auditable configuration for TD‑related models with no duplicate `efficiency_tds` definitions or conflicting feature sets.

**Steps:**

1. **Clarify the intended TD efficiency problem definition**
   - Decide with domain logic:
     - What is the canonical label (`tds_per_opportunity`, `prob_td_on_touch`, etc.)?
     - Which feature families should TD efficiency use (usage, red zone roles, team totals, pace, defense, etc.)?
   - Capture this in a short design note.

2. **Resolve the duplicate `efficiency_tds` blocks**
   - Compare the two blocks in `config/training.yaml`:
     - Feature prefixes.
     - Columns to discard.
     - Any differences in hyperparams.
   - Choose one as canonical (or merge them):
     - E.g., keep the richer feature set but drop obviously redundant or leaky prefixes (`ps_game_*`, same‑game outcomes).
   - Remove the duplicate block and update any references that rely on it.

3. **Align anytime TD model configuration with the efficiency chain**
   - Document the chain:
     - Availability → usage (targets/carries) → efficiency (yards/TD per opportunity) → anytime TD.
   - Ensure:
     - Anytime TD config uses:
       - Probabilities and expected counts from upstream problems.
       - Contextual features like team totals, game totals, pace.
     - Avoid re‑using raw same‑game stats or label proxies as features.

4. **Simplify modeling options**
   - If LightGBM isn’t used:
     - Either remove it from `models:` or clearly comment it as “experimental/off by default”.
   - Ensure `training.models_to_train` includes only the models you actually train and deploy (e.g., `['xgboost']`).

5. **Add consistency checks**
   - Test to assert no duplicate `name:` entries in `problems` config.
   - Add a small script or test that:
     - Loads all problem definitions.
     - Checks for unique names and consistent `problem_type`.

**Risks / Pitfalls:**

- Removing a duplicate block changes behavior if production unexpectedly relies on the “second” version; you’ll want to re‑run a backtest to confirm performance stays acceptable or improves.

---

**Epic T2.1 – Pre‑snap feature audit and realistic football semantics**

**Goal:** Ensure every pre‑snap feature reflects something knowable pre‑kickoff and ties clearly to real mechanisms: role, alignment, script, red zone usage, team tendencies.

**Steps:**

1. **Categorize pre‑snap features by realism**
   - For each `ps_*` (or new `ps_hist_*`/`ps_game_*`) prefix, classify:
     - **Realistic pre‑game info:** historical role, alignment tendencies, red zone share, scripted usage trends.
     - **Borderline:** things that correlate with game plan but may be noisy (e.g., early‑game pace within the current season).
     - **Unrealistic (leaky):** same‑game realized stats, direct label proxies (e.g., same‑game TD shares).
   - Document per feature family: `ps_route_participation_*`, `ps_targets_*`, `ps_scripted_*`, red zone splits, etc.

2. **Prune or rename unrealistic pre‑snap features**
   - For features that are “unrealistic” but currently used:
     - Either:
       - Remove them as inputs (preferred).
       - Or redesign them as historical aggregates (e.g., last 8 games, season to date).
   - Ensure new names reflect semantics:
     - `rz_target_share_last_8g` rather than `player_red_zone_target_share`.

3. **Re‑balance feature sets around real drivers**
   - Emphasize:
     - Team implied totals and spreads (odds).
     - Historical red zone role (`targets inside 10`, `carries inside 5`, etc. per player).
     - Positional and formation tendencies (slot vs wide vs backfield usage).
     - Pace, pass rate over expected, run/pass splits by game script.
   - De‑emphasize:
     - Features that mix realized “hot hand” from the current game with historical role.

4. **Define small, interpretable pre‑snap feature groups**
   - Create 3–5 conceptual groups:
     - `role_usage` (routes, snaps, alignment history).
     - `scripted_intent` (openers, 3rd‑down usage).
     - `scoring_context` (team totals, red zone history).
     - `game_environment` (pace, spread, weather).
   - Ensure each pre‑snap or TD model uses these groups in ways that match football logic.

5. **Add documentation**
   - Write brief docs (even in comments or a markdown file) describing:
     - What each group means.
     - How it ties to real decision‑making (e.g., “this approximates OC’s likely red zone plan”).

**Risks / Pitfalls:**

- Over‑simplifying may hurt raw accuracy; need to balance football realism with empirical performance.
- Some features may have overlapping information; regularization/hyperparameters may need retuning after pruning.

---

**Epic T2.2 – Handling low‑coverage pre‑snap tracking and baselines**

**Goal:** Make tracking coverage and baseline fallback explicit, avoiding silent “noisy filler” behavior for players/seasons without tracking.

**Steps:**

1. **Make tracking coverage visible**
   - Add indicators:
     - `has_tracking_this_season`, `has_tracking_last_3g`, etc.
   - When `_load_ps_baselines` succeeds/fails, record:
     - A flag column: `ps_baseline_source` (`actual`, `baseline`, `none`).

2. **Define a clear fallback strategy**
   - Policies for missing tracking:
     - If no tracking and no meaningful baseline:
       - Either drop those player‑game rows from pre‑snap models, or
       - Use heavily shrunk priors (e.g., depth‑chart based role) with explicit indicator flags.
   - For low‑usage players:
     - Consider hierarchical pooling (e.g., use positional averages by team/role).

3. **Update `_apply_ps_fallback` to be explicit, not silent**
   - Instead of a no‑op on empty baseline:
     - Set `ps_hist_*` to `NaN` or a defined prior and mark the row via an indicator.
   - Ensure models:
     - Either handle missing values robustly, or
     - Use the indicator to “know” that this is a low‑info case and rely more on team/position context.

4. **Stratify evaluation by tracking coverage**
   - When evaluating pre‑snap and TD models:
     - Slice metrics by `has_tracking_*` and `ps_baseline_source`.
     - Confirm that:
       - Performance on fully tracked cohorts is good.
       - Performance on low‑coverage cohorts is at least reasonable, not driven by noise.

**Risks / Pitfalls:**

- Dropping low‑coverage rows reduces sample size; may need regularization tweaks.
- Introducing NaNs requires careful handling in feature matrices and model training.

---

**Epic T2.3 – Refine scripted touches / early‑game heuristics**

**Goal:** Move from a rigid “first 15 plays” definition to something that captures how teams actually script drives and touches.

**Steps:**

1. **Analyze current scripted heuristic**
   - Use existing `offense_play_rank < 15` scripted labels to:
     - Compute per‑team distribution of scripted plays per game.
     - See whether 15 is a good cutoff across teams/seasons.
   - Look for:
     - Two‑minute drills, hurry‑up sequences that are mis‑classified.
     - Games where the first 15 plays span multiple drives with very different context.

2. **Design a more nuanced scripted definition**
   - Candidate heuristics:
     - “Scripted” = plays in first X minutes of game clock on early downs outside two‑minute/4‑minute situations.
     - Or: first N plays **per offense** excluding hurry‑up scenarios (no huddles, < 1:00 on clock).
   - Optionally:
     - Design team‑level `script_length` parameters learned from historical behavior.

3. **Adjust labels and recompute `ps_game_scripted_*`**
   - Implement improved heuristics in `player_game_level`:
     - New flag `is_scripted_play`.
     - Recompute `ps_game_scripted_targets`, `ps_game_scripted_carries`, etc.
   - Use historical data to confirm:
     - New scripted labels correlate with expected openers (WR1/WR2, goal‑line RBs, etc.).

4. **Re‑train pre‑snap models that use scripted features**
   - Update pre‑snap models that include scripted touch features:
     - Use `ps_hist_*` approximations to predict new `ps_game_scripted_*` labels.
   - Re‑evaluate:
     - How much scripted touches add over generic role metrics to anytime TD prediction.

**Risks / Pitfalls:**

- More complex heuristics may be harder to reason about; keep them as simple as possible while fixing major issues.
- Some teams may have inconsistent scripting; avoid overfitting to quirks in small samples.

---

**Epic T3.1 – Cleanup of legacy / unused code**

**Goal:** Reduce cognitive load and risk of misinterpretation by clearly marking or removing unused configs and modules.

**Steps:**

1. **Identify unused model families and configs**
   - LightGBM blocks in `config/training.yaml`.
   - Legacy MLB code under `utils/train/selective/*`, `mlb_legacy.md`, etc.
   - Any old problems in `problems` list that are not:
     - Trained.
     - Saved.
     - Used downstream in prediction.

2. **Decide policy: remove vs quarantine**
   - For NFL work, likely:
     - Move legacy configs into a `legacy/` or `archive/` section with explicit comments.
     - Or delete entirely if not referenced.

3. **Update documentation**
   - Document the active set of problems and model types:
     - E.g., “Active for NFL anytime TD pipeline: availability_active, pre_snap_routes, pre_snap_scripted_touches, usage_targets, usage_carries, efficiency_yards, efficiency_tds, anytime_td.”

4. **Add CI/Tests for config hygiene**
   - Test that:
     - All `problems.name` values are unique.
     - All referenced problems have corresponding model artifacts.
     - No model type is declared in `config` but never used anywhere.

**Risks / Pitfalls:**

- Removing code that is used in ad‑hoc scripts or by other teams. Favor quarantining with documentation over hard deletion unless you’re sure.

---

**Epic T3.2 – Observability & leakage guards**

**Goal:** Add automated checks that protect against reintroducing leakage or train/predict feature mismatches.

**Steps:**

1. **Feature parity tests**
   - For each problem:
     - Load training feature names from artifacts.
     - Load prediction feature names from the live pipeline.
     - Assert parity, with a small list of allowed differences (e.g., `game_id`, `player_id`).

2. **Leakage signature tests**
   - Maintain lists of “never features”:
     - Same‑game outcomes: `*_yards`, `*_td`, `*_touches`, `*_targets`, etc.
     - `ps_game_*` namespace.
   - Test that:
     - These columns never appear in `features_to_use` for any problem.

3. **As‑of and snapshot tests**
   - Add regression tests ensuring:
     - `decision_cutoff_ts` logic in `feature.py` and `player_game_level.py` correctly filters injury/odds/weather columns.
     - No columns with `reported_at` > `decision_cutoff_ts` survive into training matrices.

4. **Monitoring / dashboards (if applicable)**
   - For production predictions:
     - Track distributions of key features and outputs over time.
     - Alert on distribution shifts, especially for:
       - `ps_hist_*` inputs.
       - `p_active`, `p_anytime_td`.

**Risks / Pitfalls:**

- Over‑constraining tests can slow iteration; keep rules minimal but high‑impact.

---

**Epic T3.3 – Documentation of football mechanisms**

**Goal:** Make the mapping from football concepts to features and models explicit so future changes stay aligned with reality.

**Steps:**

1. **Write a short “anytime TD model design” doc**
   - Sections:
     - High‑level architecture (availability → usage → efficiency → TD).
     - Definitions of key labels (what is “anytime TD” exactly, what is “efficiency_tds”).
     - Explanation of pre‑snap models and how their predictions are used.

2. **Document feature groups and semantics**
   - For each group (role_usage, scripted_intent, scoring_context, game_environment):
     - List representative features.
     - Explain their intended connection to TD probability.

3. **Add inline references from config to docs**
   - In `config/training.yaml`:
     - Add short comments or references to the design doc sections next to each problem.
   - Optionally, generate a markdown summary from the config so docs stay in sync.

---

If you’d like, next step could be to pick one Tier‑1 epic (e.g., `ps_*` alignment or classification pipeline) and I can turn that into a more concrete implementation checklist (file‑by‑file changes, data migration steps, and specific tests to add), ready for an agent to execute.
